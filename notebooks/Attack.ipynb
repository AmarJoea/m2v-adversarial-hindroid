{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worst-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "import os\n",
    "os.chdir('/home/rcgonzal/DSC180Malware/m2v-adversarial-hindroid/')\n",
    "\n",
    "from __future__ import print_function\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from src.imbalanced_dataset_sampler.imbalanced import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alleged-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6421, 2535703)\n"
     ]
    }
   ],
   "source": [
    "with open('data/out/all-apps/hindroid-train-set/A_test.npz', 'rb') as file:\n",
    "    print(sparse.load_npz(file).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ready-pioneer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2674056, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/out/train-set/method_map.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romance-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindroidDataset(Dataset):\n",
    "    def __init__(self, features_path, labels_path, label_col='m2vDroid', transform=None):\n",
    "        '''\n",
    "        Creates a  dataset from the A matrix representation of apps and their associated labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -------------------\n",
    "        features_path: Path to A matrix in sparse format.\n",
    "        labels_path: Path to labels in csv format.\n",
    "        label_col: Default 'm2vDroid'. Useful for specifying which kernel to use for HinDroid.\n",
    "        '''\n",
    "        self.features = sparse.load_npz(os.path.join(features_path))\n",
    "        self.feature_width = self.features.shape[1]\n",
    "        features_folder = os.path.split(features_path)[0]\n",
    "        self.features_idx = list(pd.read_csv(\n",
    "            os.path.join(features_folder, 'predictions.csv'),\n",
    "            usecols=['app'], \n",
    "            squeeze=True\n",
    "        ))\n",
    "        self.transform = transform\n",
    "        \n",
    "        try:\n",
    "            self.labels = pd.read_csv(\n",
    "                labels_path, \n",
    "                usecols=['app', label_col],\n",
    "                index_col = 'app',\n",
    "                squeeze=True\n",
    "            )\n",
    "            self.labels = self.labels[self.features_idx].values # align labels with features index\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(e)\n",
    "            print('Seems like you may be trying to use a different model. This class is setup for m2vDroid by default.')\n",
    "            print('For HinDroid you must specify `label_col` as either AAT, ABAT, APAT, ABPBTAT, or APBPTAT.')\n",
    "            \n",
    "        assert (self.features.shape[0] == self.labels.size), 'Length mismatch between features and labels.'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        features = self.features[idx]\n",
    "        features = features.todense().astype('float').A\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "#         if self.transform:\n",
    "#             features = self.transform(features)\n",
    "#             labels = self.transform(labels)\n",
    "        \n",
    "#         sample = {'features': features, 'labels': labels}\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        return self.labels[idx]\n",
    "    \n",
    "def hindroid_custom_get_label(dataset, idx):\n",
    "    return dataset.get_labels(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "logical-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindroidSubstitute(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(HindroidSubstitute, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_features, 64, bias=False)\n",
    "        # Linear - how to freeze layer ^\n",
    "        # biases = false\n",
    "        self.layer_2 = nn.Linear(64, 64, bias=False)\n",
    "        self.layer_3 = nn.Linear(64, 64, bias=False)\n",
    "        self.layer_4 = nn.Linear(64, 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.relu(self.layer_3(x))\n",
    "        x = self.layer_4(x)\n",
    "        return x # logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informed-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "# model = HindroidSubstitute(dataset.feature_width).double()\n",
    "# model(dataset[-100:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assured-timothy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "The negative log likelihood loss.\n",
       "\n",
       "See :class:`~torch.nn.NLLLoss` for details.\n",
       "\n",
       "Args:\n",
       "    input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
       "        in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n",
       "        in the case of K-dimensional loss.\n",
       "    target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
       "        or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
       "        K-dimensional loss.\n",
       "    weight (Tensor, optional): a manual rescaling weight given to each\n",
       "        class. If given, has to be a Tensor of size `C`\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when reduce is ``False``. Default: ``True``\n",
       "    ignore_index (int, optional): Specifies a target value that is ignored\n",
       "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "        ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (string, optional): Specifies the reduction to apply to the output:\n",
       "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
       "        ``'mean'``: the sum of the output will be divided by the number of\n",
       "        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
       "        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> # input is of size N x C = 3 x 5\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> # each element in target has to have 0 <= value < C\n",
       "    >>> target = torch.tensor([1, 0, 4])\n",
       "    >>> output = F.nll_loss(F.log_softmax(input), target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/torch/nn/functional.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.nll_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fiscal-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, weight=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = F.log_softmax(model(data), dim=1)\n",
    "        loss = F.nll_loss(output, target, weight=weight) # do we use different loss?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        log_interval = 100\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "#         if batch_idx % args.log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "#             if args.dry_run:\n",
    "#                 break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, weight=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "#             print(output)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "#             print(output)\n",
    "            loss = F.nll_loss(output, target, weight=weight, reduction='sum').item()  # sum up batch loss\n",
    "#             print('loss: ', loss)\n",
    "            test_loss += loss\n",
    "#             print(output.argmax(dim=1, keepdim=True))\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             print(target.view_as(pred))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "natural-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HindroidSubstitute(dataset.feature_width).double()\n",
    "# weights = torch.Tensor([dataset.labels.mean() / (1-dataset.labels.mean()), 1]).double()\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "# test(model, 'cpu', test_loader, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-liechtenstein",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rocky-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# train_kwargs = {'batch_size': args.batch_size}\n",
    "# test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "#                    'shuffle': True,\n",
    "                   'pin_memory': True}\n",
    "#     train_kwargs.update(cuda_kwargs)\n",
    "#     test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "# load data (will need to be adapted as well)\n",
    "# 1) load A_test\n",
    "# 2) load labels\n",
    "# 3) Perform train-test-split\n",
    "dataset = HindroidDataset(\n",
    "    'data/out/all-apps/hindroid-train-set/A_test.npz', \n",
    "    'data/out/all-apps/hindroid-train-set/predictions.csv',\n",
    "    'AAT',\n",
    ")\n",
    "# weights = torch.Tensor([dataset.labels.mean() / (1-dataset.labels.mean()), 1]).to(device).double()\n",
    "weights = None\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=10,\n",
    "    shuffle = False,\n",
    "    sampler = ImbalancedDatasetSampler(dataset, callback_get_label = hindroid_custom_get_label),\n",
    "    **cuda_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=10,\n",
    "    shuffle = False,\n",
    "    sampler = ImbalancedDatasetSampler(dataset, callback_get_label = hindroid_custom_get_label),\n",
    "    **cuda_kwargs)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "\n",
    "model = HindroidSubstitute(dataset.feature_width).double().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "arctic-stock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6421 (0%)]\tLoss: 0.693308\n",
      "Train Epoch: 1 [1000/6421 (16%)]\tLoss: 0.547779\n",
      "Train Epoch: 1 [2000/6421 (31%)]\tLoss: 0.449607\n",
      "Train Epoch: 1 [3000/6421 (47%)]\tLoss: 0.415554\n",
      "Train Epoch: 1 [4000/6421 (62%)]\tLoss: 0.509774\n",
      "Train Epoch: 1 [5000/6421 (78%)]\tLoss: 0.452204\n",
      "Train Epoch: 1 [6000/6421 (93%)]\tLoss: 0.481490\n",
      "\n",
      "Test set: Average loss: 0.3753, Accuracy: 6267/6421 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/6421 (0%)]\tLoss: 0.375891\n",
      "Train Epoch: 2 [1000/6421 (16%)]\tLoss: 0.384969\n",
      "Train Epoch: 2 [2000/6421 (31%)]\tLoss: 0.327258\n",
      "Train Epoch: 2 [3000/6421 (47%)]\tLoss: 0.372981\n",
      "Train Epoch: 2 [4000/6421 (62%)]\tLoss: 0.341142\n",
      "Train Epoch: 2 [5000/6421 (78%)]\tLoss: 0.376461\n",
      "Train Epoch: 2 [6000/6421 (93%)]\tLoss: 0.416246\n",
      "\n",
      "Test set: Average loss: 0.3641, Accuracy: 6288/6421 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/6421 (0%)]\tLoss: 0.319061\n",
      "Train Epoch: 3 [1000/6421 (16%)]\tLoss: 0.504297\n",
      "Train Epoch: 3 [2000/6421 (31%)]\tLoss: 0.243470\n",
      "Train Epoch: 3 [3000/6421 (47%)]\tLoss: 0.199253\n",
      "Train Epoch: 3 [4000/6421 (62%)]\tLoss: 0.544423\n",
      "Train Epoch: 3 [5000/6421 (78%)]\tLoss: 0.305905\n",
      "Train Epoch: 3 [6000/6421 (93%)]\tLoss: 0.484879\n",
      "\n",
      "Test set: Average loss: 0.3617, Accuracy: 6274/6421 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/6421 (0%)]\tLoss: 0.265573\n",
      "Train Epoch: 4 [1000/6421 (16%)]\tLoss: 0.490486\n",
      "Train Epoch: 4 [2000/6421 (31%)]\tLoss: 0.406480\n",
      "Train Epoch: 4 [3000/6421 (47%)]\tLoss: 0.400873\n",
      "Train Epoch: 4 [4000/6421 (62%)]\tLoss: 0.406776\n",
      "Train Epoch: 4 [5000/6421 (78%)]\tLoss: 0.499747\n",
      "Train Epoch: 4 [6000/6421 (93%)]\tLoss: 0.422717\n",
      "\n",
      "Test set: Average loss: 0.3604, Accuracy: 6289/6421 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/6421 (0%)]\tLoss: 0.429054\n",
      "Train Epoch: 5 [1000/6421 (16%)]\tLoss: 0.469366\n",
      "Train Epoch: 5 [2000/6421 (31%)]\tLoss: 0.412700\n",
      "Train Epoch: 5 [3000/6421 (47%)]\tLoss: 0.383571\n",
      "Train Epoch: 5 [4000/6421 (62%)]\tLoss: 0.465821\n",
      "Train Epoch: 5 [5000/6421 (78%)]\tLoss: 0.521088\n",
      "Train Epoch: 5 [6000/6421 (93%)]\tLoss: 0.457039\n",
      "\n",
      "Test set: Average loss: 0.3711, Accuracy: 6271/6421 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "for epoch in range(1, 5 + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, weights)\n",
    "    test(model, device, test_loader, weights)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "industrial-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       766\n",
      "           1       1.00      0.99      1.00      5655\n",
      "\n",
      "    accuracy                           0.99      6421\n",
      "   macro avg       0.98      0.99      0.99      6421\n",
      "weighted avg       0.99      0.99      0.99      6421\n",
      "\n",
      "[[ 758    8]\n",
      " [  30 5625]]\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=10,\n",
    "    **cuda_kwargs)\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        all_preds.extend(pred.tolist())\n",
    "\n",
    "print(classification_report(dataset.labels, all_preds))\n",
    "print(confusion_matrix(dataset.labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chemical-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/out/all-apps/hindroid-train-set/NN_sub.pkl', 'wb') as file:\n",
    "    torch.save(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chicken-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/out/all-apps/hindroid-train-set/NN_sub.pkl', 'rb') as file:\n",
    "    model = torch.load(file).to(device)\n",
    "\n",
    "batch_size = 10\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    "    **cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "african-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attack.cw import to_tanh_space, from_tanh_space, L2Adversary\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hybrid-parker",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0523, 0.9477],\n",
      "        [0.0226, 0.9774],\n",
      "        [0.0336, 0.9664],\n",
      "        [0.0112, 0.9888],\n",
      "        [0.0115, 0.9885],\n",
      "        [0.2354, 0.7646],\n",
      "        [0.1143, 0.8857],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0126, 0.9874]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.1377753987150987\n",
      "batch [10] loss: 36417599.849875465\n",
      "batch [20] loss: 36417599.849875465\n",
      "batch [30] loss: 36417599.849875465\n",
      "batch [40] loss: 36417599.849875465\n",
      "batch [50] loss: 36417599.849875465\n",
      "batch [60] loss: 36417599.849875465\n",
      "batch [70] loss: 36417599.849875465\n",
      "batch [80] loss: 36417599.849875465\n",
      "batch [90] loss: 36417599.849875465\n",
      "batch [100] loss: 36417599.849875465\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 36417599.849875465\n",
      "batch [10] loss: 36417599.849875465\n",
      "batch [20] loss: 36417599.849875465\n",
      "batch [30] loss: 36417599.849875465\n",
      "batch [40] loss: 36417599.849875465\n",
      "batch [50] loss: 36417599.849875465\n",
      "batch [60] loss: 36417599.849875465\n",
      "batch [70] loss: 36417599.849875465\n",
      "batch [80] loss: 36417599.849875465\n",
      "batch [90] loss: 36417599.849875465\n",
      "batch [100] loss: 36417599.849875465\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 36417599.849875465\n",
      "batch [10] loss: 36417599.849875465\n",
      "batch [20] loss: 36417599.849875465\n",
      "batch [30] loss: 36417599.849875465\n",
      "batch [40] loss: 36417599.849875465\n",
      "batch [50] loss: 36417599.849875465\n",
      "batch [60] loss: 36417599.849875465\n",
      "batch [70] loss: 36417599.849875465\n",
      "batch [80] loss: 36417599.849875465\n",
      "batch [90] loss: 36417599.849875465\n",
      "batch [100] loss: 36417599.849875465\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 36417599.849875465\n",
      "batch [10] loss: 36417599.849875465\n",
      "batch [20] loss: 36417599.849875465\n",
      "batch [30] loss: 36417599.849875465\n",
      "batch [40] loss: 36417599.849875465\n",
      "batch [50] loss: 36417599.849875465\n",
      "batch [60] loss: 36417599.849875465\n",
      "batch [70] loss: 36417599.849875465\n",
      "batch [80] loss: 36417599.849875465\n",
      "batch [90] loss: 36417599.849875465\n",
      "batch [100] loss: 36417599.849875465\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 36417599.849875465\n",
      "batch [10] loss: 36417599.849875465\n",
      "batch [20] loss: 36417599.849875465\n",
      "batch [30] loss: 36417599.849875465\n",
      "batch [40] loss: 36417599.849875465\n",
      "batch [50] loss: 36417599.849875465\n",
      "batch [60] loss: 36417599.849875465\n",
      "batch [70] loss: 36417599.849875465\n",
      "batch [80] loss: 36417599.849875465\n",
      "batch [90] loss: 36417599.849875465\n",
      "batch [100] loss: 36417599.849875465\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1])\n",
      "Model pred:  tensor([[1.2426e-02, 9.8757e-01],\n",
      "        [8.2800e-02, 9.1720e-01],\n",
      "        [2.5860e-01, 7.4140e-01],\n",
      "        [4.5845e-03, 9.9542e-01],\n",
      "        [4.8075e-02, 9.5192e-01],\n",
      "        [1.3193e-02, 9.8681e-01],\n",
      "        [9.7678e-01, 2.3218e-02],\n",
      "        [9.9999e-01, 5.4791e-06],\n",
      "        [1.3805e-01, 8.6195e-01],\n",
      "        [8.1712e-02, 9.1829e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.061937346352185\n",
      "batch [10] loss: 36145939.13999537\n",
      "batch [20] loss: 36144730.04335371\n",
      "batch [30] loss: 36144715.353062615\n",
      "batch [40] loss: 36144700.79170013\n",
      "batch [50] loss: 36144700.790033355\n",
      "batch [60] loss: 36144700.79337478\n",
      "batch [70] loss: 36144700.79050377\n",
      "batch [80] loss: 36144700.81584637\n",
      "batch [90] loss: 36144700.79000252\n",
      "batch [100] loss: 36144700.79000547\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 36144797.749662995\n",
      "batch [10] loss: 36144700.32050124\n",
      "batch [20] loss: 36144700.32050124\n",
      "batch [30] loss: 36144700.32052669\n",
      "batch [40] loss: 36144700.32050124\n",
      "batch [50] loss: 36144700.32050124\n",
      "batch [60] loss: 36144700.32050124\n",
      "batch [70] loss: 36144700.32050124\n",
      "batch [80] loss: 36144700.32050124\n",
      "batch [90] loss: 36144700.32050124\n",
      "batch [100] loss: 36144700.32050124\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025]\n",
      "batch [0] loss: 36144700.55525165\n",
      "batch [10] loss: 36144700.555251665\n",
      "batch [20] loss: 36144700.55525165\n",
      "batch [30] loss: 36144700.55525166\n",
      "batch [40] loss: 36144700.55525166\n",
      "batch [50] loss: 36144700.55525166\n",
      "batch [60] loss: 36144700.555251844\n",
      "batch [70] loss: 36144700.55525165\n",
      "batch [80] loss: 36144700.55525289\n",
      "batch [90] loss: 36144700.55525165\n",
      "batch [100] loss: 36144700.555252284\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125]\n",
      "batch [0] loss: 36144700.67262681\n",
      "batch [10] loss: 36144700.67262681\n",
      "batch [20] loss: 36144700.67264759\n",
      "batch [30] loss: 36144700.673111305\n",
      "batch [40] loss: 36144700.67262708\n",
      "batch [50] loss: 36144700.67262695\n",
      "batch [60] loss: 36144700.67262753\n",
      "batch [70] loss: 36144700.67262682\n",
      "batch [80] loss: 36144700.672626816\n",
      "batch [90] loss: 36144700.67262681\n",
      "batch [100] loss: 36144700.67868192\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625]\n",
      "batch [0] loss: 36144700.73131442\n",
      "batch [10] loss: 36144700.73131442\n",
      "batch [20] loss: 36144700.73131451\n",
      "batch [30] loss: 36144700.73131442\n",
      "batch [40] loss: 36144700.73131442\n",
      "batch [50] loss: 36144700.73131443\n",
      "batch [60] loss: 36144700.73133289\n",
      "batch [70] loss: 36144700.73131442\n",
      "batch [80] loss: 36144700.73131442\n",
      "batch [90] loss: 36144795.71806517\n",
      "batch [100] loss: 36144700.73131442\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.4803e-01, 8.5197e-01],\n",
      "        [1.1877e-01, 8.8123e-01],\n",
      "        [7.3150e-03, 9.9268e-01],\n",
      "        [1.7037e-03, 9.9830e-01],\n",
      "        [2.2315e-04, 9.9978e-01],\n",
      "        [8.0803e-03, 9.9192e-01],\n",
      "        [7.3150e-03, 9.9268e-01],\n",
      "        [1.6687e-02, 9.8331e-01],\n",
      "        [1.1688e-02, 9.8831e-01],\n",
      "        [4.3791e-02, 9.5621e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.476347095440932\n",
      "batch [10] loss: 49961999.79404128\n",
      "batch [20] loss: 49961999.79404128\n",
      "batch [30] loss: 49961999.79404128\n",
      "batch [40] loss: 49961999.79404128\n",
      "batch [50] loss: 49961999.79404128\n",
      "batch [60] loss: 49961999.79404128\n",
      "batch [70] loss: 49961999.79404128\n",
      "batch [80] loss: 49961999.79404128\n",
      "batch [90] loss: 49961999.79404128\n",
      "batch [100] loss: 49961999.79404128\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 49961999.79404128\n",
      "batch [10] loss: 49961999.79404128\n",
      "batch [20] loss: 49961999.79404128\n",
      "batch [30] loss: 49961999.79404128\n",
      "batch [40] loss: 49961999.79404128\n",
      "batch [50] loss: 49961999.79404128\n",
      "batch [60] loss: 49961999.79404128\n",
      "batch [70] loss: 49961999.79404128\n",
      "batch [80] loss: 49961999.79404128\n",
      "batch [90] loss: 49961999.79404128\n",
      "batch [100] loss: 49961999.79404128\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 49961999.79404128\n",
      "batch [10] loss: 49961999.79404128\n",
      "batch [20] loss: 49961999.79404128\n",
      "batch [30] loss: 49961999.79404128\n",
      "batch [40] loss: 49961999.79404128\n",
      "batch [50] loss: 49961999.79404128\n",
      "batch [60] loss: 49961999.79404128\n",
      "batch [70] loss: 49961999.79404128\n",
      "batch [80] loss: 49961999.79404128\n",
      "batch [90] loss: 49961999.79404128\n",
      "batch [100] loss: 49961999.79404128\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 49961999.79404128\n",
      "batch [10] loss: 49961999.79404128\n",
      "batch [20] loss: 49961999.79404128\n",
      "batch [30] loss: 49961999.79404128\n",
      "batch [40] loss: 49961999.79404128\n",
      "batch [50] loss: 49961999.79404128\n",
      "batch [60] loss: 49961999.79404128\n",
      "batch [70] loss: 49961999.79404128\n",
      "batch [80] loss: 49961999.79404128\n",
      "batch [90] loss: 49961999.79404128\n",
      "batch [100] loss: 49961999.79404128\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 49961999.79404128\n",
      "batch [10] loss: 49961999.79404128\n",
      "batch [20] loss: 49961999.79404128\n",
      "batch [30] loss: 49961999.79404128\n",
      "batch [40] loss: 49961999.79404128\n",
      "batch [50] loss: 49961999.79404128\n",
      "batch [60] loss: 49961999.79404128\n",
      "batch [70] loss: 49961999.79404128\n",
      "batch [80] loss: 49961999.79404128\n",
      "batch [90] loss: 49961999.79404128\n",
      "batch [100] loss: 49961999.79404128\n",
      "OG Labels:  tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[6.3465e-02, 9.3654e-01],\n",
      "        [3.1745e-03, 9.9683e-01],\n",
      "        [9.9949e-01, 5.0803e-04],\n",
      "        [3.1283e-01, 6.8717e-01],\n",
      "        [1.6301e-01, 8.3699e-01],\n",
      "        [1.2155e-02, 9.8785e-01],\n",
      "        [8.7353e-01, 1.2647e-01],\n",
      "        [1.4057e-01, 8.5943e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.2138e-01, 8.7862e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.098814015932567\n",
      "batch [10] loss: 31420599.870474633\n",
      "batch [20] loss: 31420599.870474633\n",
      "batch [30] loss: 31420599.870474633\n",
      "batch [40] loss: 31420599.870474633\n",
      "batch [50] loss: 31420599.870474633\n",
      "batch [60] loss: 31420599.870474633\n",
      "batch [70] loss: 31420599.870474633\n",
      "batch [80] loss: 31420599.870474633\n",
      "batch [90] loss: 31420599.870474633\n",
      "batch [100] loss: 31420599.870474633\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 31420599.870474633\n",
      "batch [10] loss: 31420599.870474633\n",
      "batch [20] loss: 31420599.870474633\n",
      "batch [30] loss: 31420599.870474633\n",
      "batch [40] loss: 31420599.870474633\n",
      "batch [50] loss: 31420599.870474633\n",
      "batch [60] loss: 31420599.870474633\n",
      "batch [70] loss: 31420599.870474633\n",
      "batch [80] loss: 31420599.870474633\n",
      "batch [90] loss: 31420599.870474633\n",
      "batch [100] loss: 31420599.870474633\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 31420599.870474633\n",
      "batch [10] loss: 31420599.870474633\n",
      "batch [20] loss: 31420599.870474633\n",
      "batch [30] loss: 31420599.870474633\n",
      "batch [40] loss: 31420599.870474633\n",
      "batch [50] loss: 31420599.870474633\n",
      "batch [60] loss: 31420599.870474633\n",
      "batch [70] loss: 31420599.870474633\n",
      "batch [80] loss: 31420599.870474633\n",
      "batch [90] loss: 31420599.870474633\n",
      "batch [100] loss: 31420599.870474633\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 31420599.870474633\n",
      "batch [10] loss: 31420599.870474633\n",
      "batch [20] loss: 31420599.870474633\n",
      "batch [30] loss: 31420599.870474633\n",
      "batch [40] loss: 31420599.870474633\n",
      "batch [50] loss: 31420599.870474633\n",
      "batch [60] loss: 31420599.870474633\n",
      "batch [70] loss: 31420599.870474633\n",
      "batch [80] loss: 31420599.870474633\n",
      "batch [90] loss: 31420599.870474633\n",
      "batch [100] loss: 31420599.870474633\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 31420599.870474633\n",
      "batch [10] loss: 31420599.870474633\n",
      "batch [20] loss: 31420599.870474633\n",
      "batch [30] loss: 31420599.870474633\n",
      "batch [40] loss: 31420599.870474633\n",
      "batch [50] loss: 31420599.870474633\n",
      "batch [60] loss: 31420599.870474633\n",
      "batch [70] loss: 31420599.870474633\n",
      "batch [80] loss: 31420599.870474633\n",
      "batch [90] loss: 31420599.870474633\n",
      "batch [100] loss: 31420599.870474633\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0817, 0.9183],\n",
      "        [0.0264, 0.9736],\n",
      "        [0.0032, 0.9968],\n",
      "        [0.1295, 0.8705],\n",
      "        [0.0973, 0.9027],\n",
      "        [0.3066, 0.6934],\n",
      "        [0.0165, 0.9835],\n",
      "        [0.0169, 0.9831],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.1197, 0.8803]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.3064610216019332\n",
      "batch [10] loss: 40054399.83488346\n",
      "batch [20] loss: 40054399.83488346\n",
      "batch [30] loss: 40054399.83488346\n",
      "batch [40] loss: 40054399.83488346\n",
      "batch [50] loss: 40054399.83488346\n",
      "batch [60] loss: 40054399.83488346\n",
      "batch [70] loss: 40054399.83488346\n",
      "batch [80] loss: 40054399.83488346\n",
      "batch [90] loss: 40054399.83488346\n",
      "batch [100] loss: 40054399.83488346\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 40054399.83488346\n",
      "batch [10] loss: 40054399.83488346\n",
      "batch [20] loss: 40054399.83488346\n",
      "batch [30] loss: 40054399.83488346\n",
      "batch [40] loss: 40054399.83488346\n",
      "batch [50] loss: 40054399.83488346\n",
      "batch [60] loss: 40054399.83488346\n",
      "batch [70] loss: 40054399.83488346\n",
      "batch [80] loss: 40054399.83488346\n",
      "batch [90] loss: 40054399.83488346\n",
      "batch [100] loss: 40054399.83488346\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 40054399.83488346\n",
      "batch [10] loss: 40054399.83488346\n",
      "batch [20] loss: 40054399.83488346\n",
      "batch [30] loss: 40054399.83488346\n",
      "batch [40] loss: 40054399.83488346\n",
      "batch [50] loss: 40054399.83488346\n",
      "batch [60] loss: 40054399.83488346\n",
      "batch [70] loss: 40054399.83488346\n",
      "batch [80] loss: 40054399.83488346\n",
      "batch [90] loss: 40054399.83488346\n",
      "batch [100] loss: 40054399.83488346\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 40054399.83488346\n",
      "batch [10] loss: 40054399.83488346\n",
      "batch [20] loss: 40054399.83488346\n",
      "batch [30] loss: 40054399.83488346\n",
      "batch [40] loss: 40054399.83488346\n",
      "batch [50] loss: 40054399.83488346\n",
      "batch [60] loss: 40054399.83488346\n",
      "batch [70] loss: 40054399.83488346\n",
      "batch [80] loss: 40054399.83488346\n",
      "batch [90] loss: 40054399.83488346\n",
      "batch [100] loss: 40054399.83488346\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 40054399.83488346\n",
      "batch [10] loss: 40054399.83488346\n",
      "batch [20] loss: 40054399.83488346\n",
      "batch [30] loss: 40054399.83488346\n",
      "batch [40] loss: 40054399.83488346\n",
      "batch [50] loss: 40054399.83488346\n",
      "batch [60] loss: 40054399.83488346\n",
      "batch [70] loss: 40054399.83488346\n",
      "batch [80] loss: 40054399.83488346\n",
      "batch [90] loss: 40054399.83488346\n",
      "batch [100] loss: 40054399.83488346\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1])\n",
      "Model pred:  tensor([[1.1457e-01, 8.8543e-01],\n",
      "        [9.9999e-01, 1.4763e-05],\n",
      "        [1.1435e-01, 8.8565e-01],\n",
      "        [5.4802e-02, 9.4520e-01],\n",
      "        [6.8677e-02, 9.3132e-01],\n",
      "        [3.6979e-01, 6.3021e-01],\n",
      "        [8.4977e-02, 9.1502e-01],\n",
      "        [8.2800e-02, 9.1720e-01],\n",
      "        [9.9962e-01, 3.8058e-04],\n",
      "        [1.2329e-01, 8.7671e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.5819903426514044\n",
      "batch [10] loss: 25727394.222002797\n",
      "batch [20] loss: 25691253.925730444\n",
      "batch [30] loss: 25692213.004981034\n",
      "batch [40] loss: 25690029.049329996\n",
      "batch [50] loss: 25689545.149830654\n",
      "batch [60] loss: 25690918.63722661\n",
      "batch [70] loss: 25692243.463271104\n",
      "batch [80] loss: 25692494.866613302\n",
      "batch [90] loss: 25693032.659052797\n",
      "batch [100] loss: 25693399.983182132\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25693720.914604783\n",
      "batch [10] loss: 25692636.18969672\n",
      "batch [20] loss: 25692695.907771014\n",
      "batch [30] loss: 25692604.658398956\n",
      "batch [40] loss: 25692601.098408617\n",
      "batch [50] loss: 25692613.895257127\n",
      "batch [60] loss: 25692600.847907387\n",
      "batch [70] loss: 25692692.451616343\n",
      "batch [80] loss: 25692600.39134682\n",
      "batch [90] loss: 25692600.441019602\n",
      "batch [100] loss: 25692700.379047833\n",
      "Using scale consts: [0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025]\n",
      "batch [0] loss: 25692600.675905928\n",
      "batch [10] loss: 25692685.73675689\n",
      "batch [20] loss: 25692806.43594417\n",
      "batch [30] loss: 25692815.558447726\n",
      "batch [40] loss: 25692930.400908038\n",
      "batch [50] loss: 25692900.432435658\n",
      "batch [60] loss: 25693039.57221689\n",
      "batch [70] loss: 25693363.0075827\n",
      "batch [80] loss: 25693400.92494665\n",
      "batch [90] loss: 25693750.529670153\n",
      "batch [100] loss: 25693500.64005175\n",
      "Using scale consts: [0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125]\n",
      "batch [0] loss: 25693602.070827466\n",
      "batch [10] loss: 25693960.362395883\n",
      "batch [20] loss: 25694219.130180687\n",
      "batch [30] loss: 25694294.00056027\n",
      "batch [40] loss: 25694587.734293975\n",
      "batch [50] loss: 25694966.883801844\n",
      "batch [60] loss: 25695284.201099616\n",
      "batch [70] loss: 25695090.270742286\n",
      "batch [80] loss: 25695117.40015494\n",
      "batch [90] loss: 25695400.763523243\n",
      "batch [100] loss: 25695311.37104395\n",
      "Using scale consts: [0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625]\n",
      "batch [0] loss: 25695705.85132075\n",
      "batch [10] loss: 25695515.130319737\n",
      "batch [20] loss: 25695902.05791153\n",
      "batch [30] loss: 25695989.27818027\n",
      "batch [40] loss: 25695896.553795435\n",
      "batch [50] loss: 25696460.670924507\n",
      "batch [60] loss: 25696313.78729435\n",
      "batch [70] loss: 25696401.10187099\n",
      "batch [80] loss: 25696930.775961623\n",
      "batch [90] loss: 25696900.5772061\n",
      "batch [100] loss: 25697100.834226944\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0013, 0.9987],\n",
      "        [0.1128, 0.8872],\n",
      "        [0.0046, 0.9954],\n",
      "        [0.1730, 0.8270],\n",
      "        [0.0138, 0.9862],\n",
      "        [0.0183, 0.9817],\n",
      "        [0.0634, 0.9366],\n",
      "        [0.0110, 0.9890],\n",
      "        [0.0238, 0.9762],\n",
      "        [0.0073, 0.9927]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.9727808069884762\n",
      "batch [10] loss: 53869799.77793212\n",
      "batch [20] loss: 53869799.77793212\n",
      "batch [30] loss: 53869799.77793212\n",
      "batch [40] loss: 53869799.77793212\n",
      "batch [50] loss: 53869799.77793212\n",
      "batch [60] loss: 53869799.77793212\n",
      "batch [70] loss: 53869799.77793212\n",
      "batch [80] loss: 53869799.77793212\n",
      "batch [90] loss: 53869799.77793212\n",
      "batch [100] loss: 53869799.77793212\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 53869799.77793212\n",
      "batch [10] loss: 53869799.77793212\n",
      "batch [20] loss: 53869799.77793212\n",
      "batch [30] loss: 53869799.77793212\n",
      "batch [40] loss: 53869799.77793212\n",
      "batch [50] loss: 53869799.77793212\n",
      "batch [60] loss: 53869799.77793212\n",
      "batch [70] loss: 53869799.77793212\n",
      "batch [80] loss: 53869799.77793212\n",
      "batch [90] loss: 53869799.77793212\n",
      "batch [100] loss: 53869799.77793212\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 53869799.77793212\n",
      "batch [10] loss: 53869799.77793212\n",
      "batch [20] loss: 53869799.77793212\n",
      "batch [30] loss: 53869799.77793212\n",
      "batch [40] loss: 53869799.77793212\n",
      "batch [50] loss: 53869799.77793212\n",
      "batch [60] loss: 53869799.77793212\n",
      "batch [70] loss: 53869799.77793212\n",
      "batch [80] loss: 53869799.77793212\n",
      "batch [90] loss: 53869799.77793212\n",
      "batch [100] loss: 53869799.77793212\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 53869799.77793212\n",
      "batch [10] loss: 53869799.77793212\n",
      "batch [20] loss: 53869799.77793212\n",
      "batch [30] loss: 53869799.77793212\n",
      "batch [40] loss: 53869799.77793212\n",
      "batch [50] loss: 53869799.77793212\n",
      "batch [60] loss: 53869799.77793212\n",
      "batch [70] loss: 53869799.77793212\n",
      "batch [80] loss: 53869799.77793212\n",
      "batch [90] loss: 53869799.77793212\n",
      "batch [100] loss: 53869799.77793212\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 53869799.77793212\n",
      "batch [10] loss: 53869799.77793212\n",
      "batch [20] loss: 53869799.77793212\n",
      "batch [30] loss: 53869799.77793212\n",
      "batch [40] loss: 53869799.77793212\n",
      "batch [50] loss: 53869799.77793212\n",
      "batch [60] loss: 53869799.77793212\n",
      "batch [70] loss: 53869799.77793212\n",
      "batch [80] loss: 53869799.77793212\n",
      "batch [90] loss: 53869799.77793212\n",
      "batch [100] loss: 53869799.77793212\n",
      "OG Labels:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[9.9999e-01, 9.2099e-06],\n",
      "        [5.3741e-02, 9.4626e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.1898e-01, 8.8102e-01],\n",
      "        [7.1043e-03, 9.9290e-01],\n",
      "        [8.2260e-02, 9.1774e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [7.9822e-02, 9.2018e-01],\n",
      "        [8.2535e-03, 9.9175e-01],\n",
      "        [1.2647e-01, 8.7353e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.782236092534178\n",
      "batch [10] loss: 26268966.634136863\n",
      "batch [20] loss: 26266126.70396167\n",
      "batch [30] loss: 26266062.78032569\n",
      "batch [40] loss: 26266042.289863102\n",
      "batch [50] loss: 26265900.826344877\n",
      "batch [60] loss: 26265999.50849545\n",
      "batch [70] loss: 26265911.477701098\n",
      "batch [80] loss: 26265900.76798161\n",
      "batch [90] loss: 26266089.68383956\n",
      "batch [100] loss: 26266000.76404784\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 26266000.394167557\n",
      "batch [10] loss: 26266000.327880874\n",
      "batch [20] loss: 26266000.327880874\n",
      "batch [30] loss: 26266000.327880874\n",
      "batch [40] loss: 26266000.327882938\n",
      "batch [50] loss: 26266000.327880878\n",
      "batch [60] loss: 26266000.327880878\n",
      "batch [70] loss: 26266000.327880874\n",
      "batch [80] loss: 26266000.327880874\n",
      "batch [90] loss: 26266000.327880874\n",
      "batch [100] loss: 26266000.327880874\n",
      "Using scale consts: [0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 26266000.545959607\n",
      "batch [10] loss: 26266000.545959763\n",
      "batch [20] loss: 26266000.568960153\n",
      "batch [30] loss: 26266000.545959607\n",
      "batch [40] loss: 26266000.546077855\n",
      "batch [50] loss: 26266000.545962743\n",
      "batch [60] loss: 26266000.545959592\n",
      "batch [70] loss: 26266000.545959592\n",
      "batch [80] loss: 26266000.545995094\n",
      "batch [90] loss: 26266000.545959592\n",
      "batch [100] loss: 26266000.54595963\n",
      "Using scale consts: [0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 26266000.65499893\n",
      "batch [10] loss: 26266000.6549989\n",
      "batch [20] loss: 26266000.65499892\n",
      "batch [30] loss: 26266000.654998913\n",
      "batch [40] loss: 26266000.655001275\n",
      "batch [50] loss: 26266000.6549989\n",
      "batch [60] loss: 26266000.725876763\n",
      "batch [70] loss: 26266000.654999085\n",
      "batch [80] loss: 26266000.655005947\n",
      "batch [90] loss: 26266000.66474071\n",
      "batch [100] loss: 26266000.6549989\n",
      "Using scale consts: [0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 26266000.713562194\n",
      "batch [10] loss: 26266006.714848943\n",
      "batch [20] loss: 26266000.709708996\n",
      "batch [30] loss: 26266000.70951861\n",
      "batch [40] loss: 26266000.709518738\n",
      "batch [50] loss: 26266000.70951859\n",
      "batch [60] loss: 26266000.70951859\n",
      "batch [70] loss: 26266000.709518597\n",
      "batch [80] loss: 26266000.70951858\n",
      "batch [90] loss: 26266000.714046285\n",
      "batch [100] loss: 26266000.709522627\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.1813e-01, 8.8187e-01],\n",
      "        [1.1285e-01, 8.8715e-01],\n",
      "        [1.0256e-03, 9.9897e-01],\n",
      "        [7.5527e-04, 9.9924e-01],\n",
      "        [1.0000e+00, 4.6275e-06],\n",
      "        [9.0607e-02, 9.0939e-01],\n",
      "        [1.1030e-02, 9.8897e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [8.3646e-03, 9.9164e-01],\n",
      "        [1.1285e-01, 8.8715e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.648372720905575\n",
      "batch [10] loss: 41612992.47566043\n",
      "batch [20] loss: 41611511.17903893\n",
      "batch [30] loss: 41611400.83006163\n",
      "batch [40] loss: 41611522.137600034\n",
      "batch [50] loss: 41611703.31296317\n",
      "batch [60] loss: 41611500.80951284\n",
      "batch [70] loss: 41611500.80944118\n",
      "batch [80] loss: 41611614.62765357\n",
      "batch [90] loss: 41611500.80960199\n",
      "batch [100] loss: 41611500.81883408\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 41611500.38297947\n",
      "batch [10] loss: 41611500.31894528\n",
      "batch [20] loss: 41611500.31894528\n",
      "batch [30] loss: 41611500.31894529\n",
      "batch [40] loss: 41611500.318951115\n",
      "batch [50] loss: 41611500.31894528\n",
      "batch [60] loss: 41611500.31894543\n",
      "batch [70] loss: 41611500.31894528\n",
      "batch [80] loss: 41611500.31894529\n",
      "batch [90] loss: 41611500.31894528\n",
      "batch [100] loss: 41611500.31894528\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 41611500.564185634\n",
      "batch [10] loss: 41611500.78991267\n",
      "batch [20] loss: 41611500.564186014\n",
      "batch [30] loss: 41611500.56418592\n",
      "batch [40] loss: 41611500.564185634\n",
      "batch [50] loss: 41611500.56457822\n",
      "batch [60] loss: 41611500.564185634\n",
      "batch [70] loss: 41611500.56418568\n",
      "batch [80] loss: 41611500.56418589\n",
      "batch [90] loss: 41611500.56418563\n",
      "batch [100] loss: 41611500.56502276\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 41611500.68680575\n",
      "batch [10] loss: 41611500.68680593\n",
      "batch [20] loss: 41611500.686805755\n",
      "batch [30] loss: 41611500.68683675\n",
      "batch [40] loss: 41611500.686805785\n",
      "batch [50] loss: 41611500.686805755\n",
      "batch [60] loss: 41611500.686877735\n",
      "batch [70] loss: 41611500.68680575\n",
      "batch [80] loss: 41611500.68680575\n",
      "batch [90] loss: 41611600.69167544\n",
      "batch [100] loss: 41611600.68677961\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 41611600.74808104\n",
      "batch [10] loss: 41611600.748220384\n",
      "batch [20] loss: 41611600.74808978\n",
      "batch [30] loss: 41611600.748070896\n",
      "batch [40] loss: 41611600.93889591\n",
      "batch [50] loss: 41611600.74806722\n",
      "batch [60] loss: 41611600.74806725\n",
      "batch [70] loss: 41611600.74806736\n",
      "batch [80] loss: 41611600.74806722\n",
      "batch [90] loss: 41611600.749620795\n",
      "batch [100] loss: 41611600.74807435\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1143, 0.8857],\n",
      "        [0.0527, 0.9473],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0938, 0.9062],\n",
      "        [0.0460, 0.9540],\n",
      "        [0.1381, 0.8619],\n",
      "        [0.1630, 0.8370],\n",
      "        [0.1436, 0.8564],\n",
      "        [0.0569, 0.9431],\n",
      "        [0.1183, 0.8817]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.272524065610085\n",
      "batch [10] loss: 25905299.893210396\n",
      "batch [20] loss: 25905299.893210396\n",
      "batch [30] loss: 25905299.893210396\n",
      "batch [40] loss: 25905299.893210396\n",
      "batch [50] loss: 25905299.893210396\n",
      "batch [60] loss: 25905299.893210396\n",
      "batch [70] loss: 25905299.893210396\n",
      "batch [80] loss: 25905299.893210396\n",
      "batch [90] loss: 25905299.893210396\n",
      "batch [100] loss: 25905299.893210396\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25905299.893210396\n",
      "batch [10] loss: 25905299.893210396\n",
      "batch [20] loss: 25905299.893210396\n",
      "batch [30] loss: 25905299.893210396\n",
      "batch [40] loss: 25905299.893210396\n",
      "batch [50] loss: 25905299.893210396\n",
      "batch [60] loss: 25905299.893210396\n",
      "batch [70] loss: 25905299.893210396\n",
      "batch [80] loss: 25905299.893210396\n",
      "batch [90] loss: 25905299.893210396\n",
      "batch [100] loss: 25905299.893210396\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 25905299.893210396\n",
      "batch [10] loss: 25905299.893210396\n",
      "batch [20] loss: 25905299.893210396\n",
      "batch [30] loss: 25905299.893210396\n",
      "batch [40] loss: 25905299.893210396\n",
      "batch [50] loss: 25905299.893210396\n",
      "batch [60] loss: 25905299.893210396\n",
      "batch [70] loss: 25905299.893210396\n",
      "batch [80] loss: 25905299.893210396\n",
      "batch [90] loss: 25905299.893210396\n",
      "batch [100] loss: 25905299.893210396\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 25905299.893210396\n",
      "batch [10] loss: 25905299.893210396\n",
      "batch [20] loss: 25905299.893210396\n",
      "batch [30] loss: 25905299.893210396\n",
      "batch [40] loss: 25905299.893210396\n",
      "batch [50] loss: 25905299.893210396\n",
      "batch [60] loss: 25905299.893210396\n",
      "batch [70] loss: 25905299.893210396\n",
      "batch [80] loss: 25905299.893210396\n",
      "batch [90] loss: 25905299.893210396\n",
      "batch [100] loss: 25905299.893210396\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 25905299.893210396\n",
      "batch [10] loss: 25905299.893210396\n",
      "batch [20] loss: 25905299.893210396\n",
      "batch [30] loss: 25905299.893210396\n",
      "batch [40] loss: 25905299.893210396\n",
      "batch [50] loss: 25905299.893210396\n",
      "batch [60] loss: 25905299.893210396\n",
      "batch [70] loss: 25905299.893210396\n",
      "batch [80] loss: 25905299.893210396\n",
      "batch [90] loss: 25905299.893210396\n",
      "batch [100] loss: 25905299.893210396\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.2539e-01, 8.7461e-01],\n",
      "        [9.3514e-01, 6.4863e-02],\n",
      "        [2.1786e-03, 9.9782e-01],\n",
      "        [8.2683e-03, 9.9173e-01],\n",
      "        [3.2505e-01, 6.7495e-01],\n",
      "        [1.0579e-02, 9.8942e-01],\n",
      "        [1.5108e-04, 9.9985e-01],\n",
      "        [2.6814e-03, 9.9732e-01],\n",
      "        [3.1065e-03, 9.9689e-01],\n",
      "        [1.1176e-01, 8.8824e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.335382523772402\n",
      "batch [10] loss: 48502099.80005944\n",
      "batch [20] loss: 48502099.80005944\n",
      "batch [30] loss: 48502099.80005944\n",
      "batch [40] loss: 48502099.80005944\n",
      "batch [50] loss: 48502099.80005944\n",
      "batch [60] loss: 48502099.80005944\n",
      "batch [70] loss: 48502099.80005944\n",
      "batch [80] loss: 48502099.80005944\n",
      "batch [90] loss: 48502099.80005944\n",
      "batch [100] loss: 48502099.80005944\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 48502099.80005944\n",
      "batch [10] loss: 48502099.80005944\n",
      "batch [20] loss: 48502099.80005944\n",
      "batch [30] loss: 48502099.80005944\n",
      "batch [40] loss: 48502099.80005944\n",
      "batch [50] loss: 48502099.80005944\n",
      "batch [60] loss: 48502099.80005944\n",
      "batch [70] loss: 48502099.80005944\n",
      "batch [80] loss: 48502099.80005944\n",
      "batch [90] loss: 48502099.80005944\n",
      "batch [100] loss: 48502099.80005944\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 48502099.80005944\n",
      "batch [10] loss: 48502099.80005944\n",
      "batch [20] loss: 48502099.80005944\n",
      "batch [30] loss: 48502099.80005944\n",
      "batch [40] loss: 48502099.80005944\n",
      "batch [50] loss: 48502099.80005944\n",
      "batch [60] loss: 48502099.80005944\n",
      "batch [70] loss: 48502099.80005944\n",
      "batch [80] loss: 48502099.80005944\n",
      "batch [90] loss: 48502099.80005944\n",
      "batch [100] loss: 48502099.80005944\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 48502099.80005944\n",
      "batch [10] loss: 48502099.80005944\n",
      "batch [20] loss: 48502099.80005944\n",
      "batch [30] loss: 48502099.80005944\n",
      "batch [40] loss: 48502099.80005944\n",
      "batch [50] loss: 48502099.80005944\n",
      "batch [60] loss: 48502099.80005944\n",
      "batch [70] loss: 48502099.80005944\n",
      "batch [80] loss: 48502099.80005944\n",
      "batch [90] loss: 48502099.80005944\n",
      "batch [100] loss: 48502099.80005944\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 48502099.80005944\n",
      "batch [10] loss: 48502099.80005944\n",
      "batch [20] loss: 48502099.80005944\n",
      "batch [30] loss: 48502099.80005944\n",
      "batch [40] loss: 48502099.80005944\n",
      "batch [50] loss: 48502099.80005944\n",
      "batch [60] loss: 48502099.80005944\n",
      "batch [70] loss: 48502099.80005944\n",
      "batch [80] loss: 48502099.80005944\n",
      "batch [90] loss: 48502099.80005944\n",
      "batch [100] loss: 48502099.80005944\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.9002, 0.0998],\n",
      "        [0.1509, 0.8491],\n",
      "        [0.1381, 0.8619],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.1126, 0.8874],\n",
      "        [0.0591, 0.9409],\n",
      "        [0.0055, 0.9945],\n",
      "        [0.0057, 0.9943],\n",
      "        [0.0107, 0.9893],\n",
      "        [0.0122, 0.9878]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.468135387679429\n",
      "batch [10] loss: 42378699.825301975\n",
      "batch [20] loss: 42378699.825301975\n",
      "batch [30] loss: 42378699.825301975\n",
      "batch [40] loss: 42378699.825301975\n",
      "batch [50] loss: 42378699.825301975\n",
      "batch [60] loss: 42378699.825301975\n",
      "batch [70] loss: 42378699.825301975\n",
      "batch [80] loss: 42378699.825301975\n",
      "batch [90] loss: 42378699.825301975\n",
      "batch [100] loss: 42378699.825301975\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 42378699.825301975\n",
      "batch [10] loss: 42378699.825301975\n",
      "batch [20] loss: 42378699.825301975\n",
      "batch [30] loss: 42378699.825301975\n",
      "batch [40] loss: 42378699.825301975\n",
      "batch [50] loss: 42378699.825301975\n",
      "batch [60] loss: 42378699.825301975\n",
      "batch [70] loss: 42378699.825301975\n",
      "batch [80] loss: 42378699.825301975\n",
      "batch [90] loss: 42378699.825301975\n",
      "batch [100] loss: 42378699.825301975\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 42378699.825301975\n",
      "batch [10] loss: 42378699.825301975\n",
      "batch [20] loss: 42378699.825301975\n",
      "batch [30] loss: 42378699.825301975\n",
      "batch [40] loss: 42378699.825301975\n",
      "batch [50] loss: 42378699.825301975\n",
      "batch [60] loss: 42378699.825301975\n",
      "batch [70] loss: 42378699.825301975\n",
      "batch [80] loss: 42378699.825301975\n",
      "batch [90] loss: 42378699.825301975\n",
      "batch [100] loss: 42378699.825301975\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 42378699.825301975\n",
      "batch [10] loss: 42378699.825301975\n",
      "batch [20] loss: 42378699.825301975\n",
      "batch [30] loss: 42378699.825301975\n",
      "batch [40] loss: 42378699.825301975\n",
      "batch [50] loss: 42378699.825301975\n",
      "batch [60] loss: 42378699.825301975\n",
      "batch [70] loss: 42378699.825301975\n",
      "batch [80] loss: 42378699.825301975\n",
      "batch [90] loss: 42378699.825301975\n",
      "batch [100] loss: 42378699.825301975\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 42378699.825301975\n",
      "batch [10] loss: 42378699.825301975\n",
      "batch [20] loss: 42378699.825301975\n",
      "batch [30] loss: 42378699.825301975\n",
      "batch [40] loss: 42378699.825301975\n",
      "batch [50] loss: 42378699.825301975\n",
      "batch [60] loss: 42378699.825301975\n",
      "batch [70] loss: 42378699.825301975\n",
      "batch [80] loss: 42378699.825301975\n",
      "batch [90] loss: 42378699.825301975\n",
      "batch [100] loss: 42378699.825301975\n",
      "OG Labels:  tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.1285e-01, 8.8715e-01],\n",
      "        [1.1030e-02, 9.8897e-01],\n",
      "        [8.0803e-03, 9.9192e-01],\n",
      "        [9.9970e-01, 3.0076e-04],\n",
      "        [1.0375e-02, 9.8963e-01],\n",
      "        [2.5863e-01, 7.4137e-01],\n",
      "        [4.3791e-02, 9.5621e-01],\n",
      "        [6.9873e-03, 9.9301e-01],\n",
      "        [3.1065e-03, 9.9689e-01],\n",
      "        [3.1745e-03, 9.9683e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.464883610674649\n",
      "batch [10] loss: 39104660.499021664\n",
      "batch [20] loss: 39085664.97792102\n",
      "batch [30] loss: 39084229.98065515\n",
      "batch [40] loss: 39084432.771222696\n",
      "batch [50] loss: 39084483.517138764\n",
      "batch [60] loss: 39085546.53414008\n",
      "batch [70] loss: 39085246.5392635\n",
      "batch [80] loss: 39085500.624296345\n",
      "batch [90] loss: 39086066.195097625\n",
      "batch [100] loss: 39085976.35575913\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 39086205.97390008\n",
      "batch [10] loss: 39085580.49380774\n",
      "batch [20] loss: 39085500.00010071\n",
      "batch [30] loss: 39085504.20397385\n",
      "batch [40] loss: 39085499.99428389\n",
      "batch [50] loss: 39085500.22363287\n",
      "batch [60] loss: 39085500.017588034\n",
      "batch [70] loss: 39085499.992714725\n",
      "batch [80] loss: 39085505.15634668\n",
      "batch [90] loss: 39085499.99262962\n",
      "batch [100] loss: 39085499.99262963\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 39085530.79758546\n",
      "batch [10] loss: 39085800.067431256\n",
      "batch [20] loss: 39085955.966626346\n",
      "batch [30] loss: 39085900.08440632\n",
      "batch [40] loss: 39085867.04919624\n",
      "batch [50] loss: 39085800.26968705\n",
      "batch [60] loss: 39085800.07914181\n",
      "batch [70] loss: 39085875.001344725\n",
      "batch [80] loss: 39085988.50159621\n",
      "batch [90] loss: 39086098.216534466\n",
      "batch [100] loss: 39085900.070293784\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 39086097.95256443\n",
      "batch [10] loss: 39086095.293341644\n",
      "batch [20] loss: 39086146.443406194\n",
      "batch [30] loss: 39086101.7634252\n",
      "batch [40] loss: 39086276.364555165\n",
      "batch [50] loss: 39086204.1408662\n",
      "batch [60] loss: 39086489.63958509\n",
      "batch [70] loss: 39086101.07138894\n",
      "batch [80] loss: 39086200.103374965\n",
      "batch [90] loss: 39086498.29588438\n",
      "batch [100] loss: 39086500.10854547\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 39086400.139682345\n",
      "batch [10] loss: 39086766.91394264\n",
      "batch [20] loss: 39086700.49356332\n",
      "batch [30] loss: 39086610.6711646\n",
      "batch [40] loss: 39087028.66110768\n",
      "batch [50] loss: 39087000.80857622\n",
      "batch [60] loss: 39087207.24905039\n",
      "batch [70] loss: 39087372.19984373\n",
      "batch [80] loss: 39087383.027467065\n",
      "batch [90] loss: 39087600.11009946\n",
      "batch [100] loss: 39087699.08398297\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0121, 0.9879],\n",
      "        [0.9741, 0.0259],\n",
      "        [0.0875, 0.9125],\n",
      "        [0.1381, 0.8619],\n",
      "        [0.0088, 0.9912],\n",
      "        [0.0014, 0.9986],\n",
      "        [0.0262, 0.9738],\n",
      "        [0.0032, 0.9968],\n",
      "        [0.0478, 0.9522],\n",
      "        [0.0148, 0.9852]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.005132336611039\n",
      "batch [10] loss: 46315799.80907204\n",
      "batch [20] loss: 46315799.80907204\n",
      "batch [30] loss: 46315799.80907204\n",
      "batch [40] loss: 46315799.80907204\n",
      "batch [50] loss: 46315799.80907204\n",
      "batch [60] loss: 46315799.80907204\n",
      "batch [70] loss: 46315799.80907204\n",
      "batch [80] loss: 46315799.80907204\n",
      "batch [90] loss: 46315799.80907204\n",
      "batch [100] loss: 46315799.80907204\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 46315799.80907204\n",
      "batch [10] loss: 46315799.80907204\n",
      "batch [20] loss: 46315799.80907204\n",
      "batch [30] loss: 46315799.80907204\n",
      "batch [40] loss: 46315799.80907204\n",
      "batch [50] loss: 46315799.80907204\n",
      "batch [60] loss: 46315799.80907204\n",
      "batch [70] loss: 46315799.80907204\n",
      "batch [80] loss: 46315799.80907204\n",
      "batch [90] loss: 46315799.80907204\n",
      "batch [100] loss: 46315799.80907204\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 46315799.80907204\n",
      "batch [10] loss: 46315799.80907204\n",
      "batch [20] loss: 46315799.80907204\n",
      "batch [30] loss: 46315799.80907204\n",
      "batch [40] loss: 46315799.80907204\n",
      "batch [50] loss: 46315799.80907204\n",
      "batch [60] loss: 46315799.80907204\n",
      "batch [70] loss: 46315799.80907204\n",
      "batch [80] loss: 46315799.80907204\n",
      "batch [90] loss: 46315799.80907204\n",
      "batch [100] loss: 46315799.80907204\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 46315799.80907204\n",
      "batch [10] loss: 46315799.80907204\n",
      "batch [20] loss: 46315799.80907204\n",
      "batch [30] loss: 46315799.80907204\n",
      "batch [40] loss: 46315799.80907204\n",
      "batch [50] loss: 46315799.80907204\n",
      "batch [60] loss: 46315799.80907204\n",
      "batch [70] loss: 46315799.80907204\n",
      "batch [80] loss: 46315799.80907204\n",
      "batch [90] loss: 46315799.80907204\n",
      "batch [100] loss: 46315799.80907204\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 46315799.80907204\n",
      "batch [10] loss: 46315799.80907204\n",
      "batch [20] loss: 46315799.80907204\n",
      "batch [30] loss: 46315799.80907204\n",
      "batch [40] loss: 46315799.80907204\n",
      "batch [50] loss: 46315799.80907204\n",
      "batch [60] loss: 46315799.80907204\n",
      "batch [70] loss: 46315799.80907204\n",
      "batch [80] loss: 46315799.80907204\n",
      "batch [90] loss: 46315799.80907204\n",
      "batch [100] loss: 46315799.80907204\n",
      "OG Labels:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[9.9998e-01, 2.4886e-05],\n",
      "        [1.1010e-02, 9.8899e-01],\n",
      "        [3.5691e-03, 9.9643e-01],\n",
      "        [1.7782e-01, 8.2218e-01],\n",
      "        [2.6995e-02, 9.7301e-01],\n",
      "        [1.6019e-01, 8.3981e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [9.3783e-02, 9.0622e-01],\n",
      "        [2.3616e-01, 7.6384e-01],\n",
      "        [2.6105e-02, 9.7389e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.698425514736285\n",
      "batch [10] loss: 35119789.49534065\n",
      "batch [20] loss: 35109179.56931981\n",
      "batch [30] loss: 35107279.39905779\n",
      "batch [40] loss: 35107239.9508836\n",
      "batch [50] loss: 35106999.20782214\n",
      "batch [60] loss: 35106873.14119987\n",
      "batch [70] loss: 35107150.59183499\n",
      "batch [80] loss: 35107492.067236096\n",
      "batch [90] loss: 35107183.205863014\n",
      "batch [100] loss: 35107091.13456862\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 35107001.1530219\n",
      "batch [10] loss: 35107000.183634795\n",
      "batch [20] loss: 35107000.186982505\n",
      "batch [30] loss: 35107000.183932625\n",
      "batch [40] loss: 35107000.18361905\n",
      "batch [50] loss: 35107000.18361917\n",
      "batch [60] loss: 35107000.18361946\n",
      "batch [70] loss: 35107000.18361907\n",
      "batch [80] loss: 35107000.18361904\n",
      "batch [90] loss: 35107000.183619045\n",
      "batch [100] loss: 35107000.18362268\n",
      "Using scale consts: [0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 35107000.34778964\n",
      "batch [10] loss: 35107066.12980545\n",
      "batch [20] loss: 35107000.45491472\n",
      "batch [30] loss: 35107021.21172845\n",
      "batch [40] loss: 35107000.347849764\n",
      "batch [50] loss: 35107000.34784528\n",
      "batch [60] loss: 35107000.36579129\n",
      "batch [70] loss: 35107100.34491277\n",
      "batch [80] loss: 35107019.99761015\n",
      "batch [90] loss: 35107000.34778978\n",
      "batch [100] loss: 35107000.347789824\n",
      "Using scale consts: [0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 35107000.67492295\n",
      "batch [10] loss: 35107000.43685298\n",
      "batch [20] loss: 35107000.4298849\n",
      "batch [30] loss: 35107300.103778735\n",
      "batch [40] loss: 35107242.07513648\n",
      "batch [50] loss: 35107300.42974565\n",
      "batch [60] loss: 35107400.090994924\n",
      "batch [70] loss: 35107303.18419279\n",
      "batch [80] loss: 35107305.21898642\n",
      "batch [90] loss: 35107400.421361715\n",
      "batch [100] loss: 35107307.126683176\n",
      "Using scale consts: [0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 35107300.48394061\n",
      "batch [10] loss: 35107346.172583334\n",
      "batch [20] loss: 35107377.98182121\n",
      "batch [30] loss: 35107300.47079979\n",
      "batch [40] loss: 35107300.680717684\n",
      "batch [50] loss: 35107465.4153453\n",
      "batch [60] loss: 35107402.467327476\n",
      "batch [70] loss: 35107442.20297371\n",
      "batch [80] loss: 35107400.470770404\n",
      "batch [90] loss: 35107301.42868556\n",
      "batch [100] loss: 35107300.470796406\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0823, 0.9177],\n",
      "        [0.1300, 0.8700],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.1279, 0.8721],\n",
      "        [0.0808, 0.9192],\n",
      "        [0.0110, 0.9890],\n",
      "        [0.1146, 0.8854],\n",
      "        [0.0073, 0.9927],\n",
      "        [0.1128, 0.8872],\n",
      "        [0.1279, 0.8721]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.890756240548485\n",
      "batch [10] loss: 35325899.854375795\n",
      "batch [20] loss: 35325899.854375795\n",
      "batch [30] loss: 35325899.854375795\n",
      "batch [40] loss: 35325899.854375795\n",
      "batch [50] loss: 35325899.854375795\n",
      "batch [60] loss: 35325899.854375795\n",
      "batch [70] loss: 35325899.854375795\n",
      "batch [80] loss: 35325899.854375795\n",
      "batch [90] loss: 35325899.854375795\n",
      "batch [100] loss: 35325899.854375795\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 35325899.854375795\n",
      "batch [10] loss: 35325899.854375795\n",
      "batch [20] loss: 35325899.854375795\n",
      "batch [30] loss: 35325899.854375795\n",
      "batch [40] loss: 35325899.854375795\n",
      "batch [50] loss: 35325899.854375795\n",
      "batch [60] loss: 35325899.854375795\n",
      "batch [70] loss: 35325899.854375795\n",
      "batch [80] loss: 35325899.854375795\n",
      "batch [90] loss: 35325899.854375795\n",
      "batch [100] loss: 35325899.854375795\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 35325899.854375795\n",
      "batch [10] loss: 35325899.854375795\n",
      "batch [20] loss: 35325899.854375795\n",
      "batch [30] loss: 35325899.854375795\n",
      "batch [40] loss: 35325899.854375795\n",
      "batch [50] loss: 35325899.854375795\n",
      "batch [60] loss: 35325899.854375795\n",
      "batch [70] loss: 35325899.854375795\n",
      "batch [80] loss: 35325899.854375795\n",
      "batch [90] loss: 35325899.854375795\n",
      "batch [100] loss: 35325899.854375795\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 35325899.854375795\n",
      "batch [10] loss: 35325899.854375795\n",
      "batch [20] loss: 35325899.854375795\n",
      "batch [30] loss: 35325899.854375795\n",
      "batch [40] loss: 35325899.854375795\n",
      "batch [50] loss: 35325899.854375795\n",
      "batch [60] loss: 35325899.854375795\n",
      "batch [70] loss: 35325899.854375795\n",
      "batch [80] loss: 35325899.854375795\n",
      "batch [90] loss: 35325899.854375795\n",
      "batch [100] loss: 35325899.854375795\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 35325899.854375795\n",
      "batch [10] loss: 35325899.854375795\n",
      "batch [20] loss: 35325899.854375795\n",
      "batch [30] loss: 35325899.854375795\n",
      "batch [40] loss: 35325899.854375795\n",
      "batch [50] loss: 35325899.854375795\n",
      "batch [60] loss: 35325899.854375795\n",
      "batch [70] loss: 35325899.854375795\n",
      "batch [80] loss: 35325899.854375795\n",
      "batch [90] loss: 35325899.854375795\n",
      "batch [100] loss: 35325899.854375795\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1])\n",
      "Model pred:  tensor([[1.2697e-02, 9.8730e-01],\n",
      "        [9.9852e-01, 1.4804e-03],\n",
      "        [2.4726e-01, 7.5274e-01],\n",
      "        [8.9604e-03, 9.9104e-01],\n",
      "        [8.2260e-02, 9.1774e-01],\n",
      "        [3.2178e-03, 9.9678e-01],\n",
      "        [8.3414e-02, 9.1659e-01],\n",
      "        [9.9996e-01, 4.0901e-05],\n",
      "        [1.3805e-01, 8.6195e-01],\n",
      "        [4.8876e-03, 9.9511e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.448349960177784\n",
      "batch [10] loss: 31768850.862978805\n",
      "batch [20] loss: 31762554.17135129\n",
      "batch [30] loss: 31762102.006209046\n",
      "batch [40] loss: 31761951.93597683\n",
      "batch [50] loss: 31761403.89997583\n",
      "batch [60] loss: 31761424.330550835\n",
      "batch [70] loss: 31761629.284219198\n",
      "batch [80] loss: 31761500.58916906\n",
      "batch [90] loss: 31761410.020851914\n",
      "batch [100] loss: 31761443.92340602\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 31761497.378975548\n",
      "batch [10] loss: 31761400.210689835\n",
      "batch [20] loss: 31761400.21068985\n",
      "batch [30] loss: 31761400.21068985\n",
      "batch [40] loss: 31761400.210689835\n",
      "batch [50] loss: 31761400.210689835\n",
      "batch [60] loss: 31761400.210689835\n",
      "batch [70] loss: 31761400.210693162\n",
      "batch [80] loss: 31761400.210689835\n",
      "batch [90] loss: 31761400.21069231\n",
      "batch [100] loss: 31761400.21859935\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025]\n",
      "batch [0] loss: 31761400.381499887\n",
      "batch [10] loss: 31761400.46044626\n",
      "batch [20] loss: 31761400.381507203\n",
      "batch [30] loss: 31761400.381499887\n",
      "batch [40] loss: 31761409.723237377\n",
      "batch [50] loss: 31761400.381515887\n",
      "batch [60] loss: 31761400.390551485\n",
      "batch [70] loss: 31761400.38149991\n",
      "batch [80] loss: 31761400.381509587\n",
      "batch [90] loss: 31761400.381499894\n",
      "batch [100] loss: 31761400.3814999\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125]\n",
      "batch [0] loss: 31761400.466904886\n",
      "batch [10] loss: 31761400.485441215\n",
      "batch [20] loss: 31761420.00607376\n",
      "batch [30] loss: 31761400.480062544\n",
      "batch [40] loss: 31761400.467662256\n",
      "batch [50] loss: 31761510.506847084\n",
      "batch [60] loss: 31761500.466861613\n",
      "batch [70] loss: 31761500.466859993\n",
      "batch [80] loss: 31761500.466960352\n",
      "batch [90] loss: 31761500.46697727\n",
      "batch [100] loss: 31761500.467277035\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625]\n",
      "batch [0] loss: 31761511.113470238\n",
      "batch [10] loss: 31761598.810278434\n",
      "batch [20] loss: 31761500.50955926\n",
      "batch [30] loss: 31761600.118327495\n",
      "batch [40] loss: 31761500.51005073\n",
      "batch [50] loss: 31761500.633201253\n",
      "batch [60] loss: 31761573.12095701\n",
      "batch [70] loss: 31761600.509496126\n",
      "batch [80] loss: 31761500.50955967\n",
      "batch [90] loss: 31761500.517918736\n",
      "batch [100] loss: 31761500.509560134\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0427, 0.9573],\n",
      "        [0.2507, 0.7493],\n",
      "        [0.0786, 0.9214],\n",
      "        [0.1237, 0.8763],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0928, 0.9072],\n",
      "        [0.1183, 0.8817],\n",
      "        [0.3393, 0.6607],\n",
      "        [0.0277, 0.9723]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.197401764514421\n",
      "batch [10] loss: 34118199.8593543\n",
      "batch [20] loss: 34118199.8593543\n",
      "batch [30] loss: 34118199.8593543\n",
      "batch [40] loss: 34118199.8593543\n",
      "batch [50] loss: 34118199.8593543\n",
      "batch [60] loss: 34118199.8593543\n",
      "batch [70] loss: 34118199.8593543\n",
      "batch [80] loss: 34118199.8593543\n",
      "batch [90] loss: 34118199.8593543\n",
      "batch [100] loss: 34118199.8593543\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 34118199.8593543\n",
      "batch [10] loss: 34118199.8593543\n",
      "batch [20] loss: 34118199.8593543\n",
      "batch [30] loss: 34118199.8593543\n",
      "batch [40] loss: 34118199.8593543\n",
      "batch [50] loss: 34118199.8593543\n",
      "batch [60] loss: 34118199.8593543\n",
      "batch [70] loss: 34118199.8593543\n",
      "batch [80] loss: 34118199.8593543\n",
      "batch [90] loss: 34118199.8593543\n",
      "batch [100] loss: 34118199.8593543\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 34118199.8593543\n",
      "batch [10] loss: 34118199.8593543\n",
      "batch [20] loss: 34118199.8593543\n",
      "batch [30] loss: 34118199.8593543\n",
      "batch [40] loss: 34118199.8593543\n",
      "batch [50] loss: 34118199.8593543\n",
      "batch [60] loss: 34118199.8593543\n",
      "batch [70] loss: 34118199.8593543\n",
      "batch [80] loss: 34118199.8593543\n",
      "batch [90] loss: 34118199.8593543\n",
      "batch [100] loss: 34118199.8593543\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 34118199.8593543\n",
      "batch [10] loss: 34118199.8593543\n",
      "batch [20] loss: 34118199.8593543\n",
      "batch [30] loss: 34118199.8593543\n",
      "batch [40] loss: 34118199.8593543\n",
      "batch [50] loss: 34118199.8593543\n",
      "batch [60] loss: 34118199.8593543\n",
      "batch [70] loss: 34118199.8593543\n",
      "batch [80] loss: 34118199.8593543\n",
      "batch [90] loss: 34118199.8593543\n",
      "batch [100] loss: 34118199.8593543\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 34118199.8593543\n",
      "batch [10] loss: 34118199.8593543\n",
      "batch [20] loss: 34118199.8593543\n",
      "batch [30] loss: 34118199.8593543\n",
      "batch [40] loss: 34118199.8593543\n",
      "batch [50] loss: 34118199.8593543\n",
      "batch [60] loss: 34118199.8593543\n",
      "batch [70] loss: 34118199.8593543\n",
      "batch [80] loss: 34118199.8593543\n",
      "batch [90] loss: 34118199.8593543\n",
      "batch [100] loss: 34118199.8593543\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "Model pred:  tensor([[1.3805e-01, 8.6195e-01],\n",
      "        [3.1745e-03, 9.9683e-01],\n",
      "        [2.5759e-02, 9.7424e-01],\n",
      "        [1.0856e-02, 9.8914e-01],\n",
      "        [1.2947e-01, 8.7053e-01],\n",
      "        [1.6266e-01, 8.3734e-01],\n",
      "        [7.4216e-02, 9.2578e-01],\n",
      "        [9.8868e-01, 1.1324e-02],\n",
      "        [9.9984e-01, 1.6265e-04],\n",
      "        [9.9888e-01, 1.1217e-03]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.177878716978915\n",
      "batch [10] loss: 29963081.18049849\n",
      "batch [20] loss: 29922943.63946935\n",
      "batch [30] loss: 29919878.266299546\n",
      "batch [40] loss: 29919001.736982815\n",
      "batch [50] loss: 29918692.05050809\n",
      "batch [60] loss: 29919444.564984396\n",
      "batch [70] loss: 29918905.237630486\n",
      "batch [80] loss: 29919650.80400473\n",
      "batch [90] loss: 29920237.705232263\n",
      "batch [100] loss: 29921306.359995596\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 29920825.70432989\n",
      "batch [10] loss: 29920016.72329485\n",
      "batch [20] loss: 29920000.0967353\n",
      "batch [30] loss: 29920099.790171932\n",
      "batch [40] loss: 29920024.64339991\n",
      "batch [50] loss: 29920000.08500635\n",
      "batch [60] loss: 29920098.539854154\n",
      "batch [70] loss: 29920001.93282155\n",
      "batch [80] loss: 29920000.08520127\n",
      "batch [90] loss: 29920000.085556\n",
      "batch [100] loss: 29920000.0874591\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.07500000000000001]\n",
      "batch [0] loss: 29920000.40261899\n",
      "batch [10] loss: 29920320.484412175\n",
      "batch [20] loss: 29920359.123217184\n",
      "batch [30] loss: 29920491.559552442\n",
      "batch [40] loss: 29920457.670122467\n",
      "batch [50] loss: 29920303.061637953\n",
      "batch [60] loss: 29920502.574623317\n",
      "batch [70] loss: 29920487.58748533\n",
      "batch [80] loss: 29920317.562741324\n",
      "batch [90] loss: 29920506.842575006\n",
      "batch [100] loss: 29920645.29140717\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.08750000000000001]\n",
      "batch [0] loss: 29920603.30516103\n",
      "batch [10] loss: 29920791.503366888\n",
      "batch [20] loss: 29921246.86450824\n",
      "batch [30] loss: 29921092.564872444\n",
      "batch [40] loss: 29921065.641867943\n",
      "batch [50] loss: 29921250.039093517\n",
      "batch [60] loss: 29921100.241267815\n",
      "batch [70] loss: 29921211.026217457\n",
      "batch [80] loss: 29921500.222031824\n",
      "batch [90] loss: 29921512.389706336\n",
      "batch [100] loss: 29921539.647517618\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.09375]\n",
      "batch [0] loss: 29921604.84184777\n",
      "batch [10] loss: 29921900.26583212\n",
      "batch [20] loss: 29921900.173508108\n",
      "batch [30] loss: 29922176.292642403\n",
      "batch [40] loss: 29922103.872938495\n",
      "batch [50] loss: 29922151.33476884\n",
      "batch [60] loss: 29922379.721619066\n",
      "batch [70] loss: 29922500.170901477\n",
      "batch [80] loss: 29922645.523470614\n",
      "batch [90] loss: 29922804.996523738\n",
      "batch [100] loss: 29922662.688199908\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0817, 0.9183],\n",
      "        [0.0023, 0.9977],\n",
      "        [0.0032, 0.9968],\n",
      "        [0.0063, 0.9937],\n",
      "        [0.1205, 0.8795],\n",
      "        [0.1381, 0.8619],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.1381, 0.8619],\n",
      "        [0.0092, 0.9908],\n",
      "        [0.0122, 0.9878]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.64601604024174\n",
      "batch [10] loss: 37725199.844485134\n",
      "batch [20] loss: 37725199.844485134\n",
      "batch [30] loss: 37725199.844485134\n",
      "batch [40] loss: 37725199.844485134\n",
      "batch [50] loss: 37725199.844485134\n",
      "batch [60] loss: 37725199.844485134\n",
      "batch [70] loss: 37725199.844485134\n",
      "batch [80] loss: 37725199.844485134\n",
      "batch [90] loss: 37725199.844485134\n",
      "batch [100] loss: 37725199.844485134\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 37725199.844485134\n",
      "batch [10] loss: 37725199.844485134\n",
      "batch [20] loss: 37725199.844485134\n",
      "batch [30] loss: 37725199.844485134\n",
      "batch [40] loss: 37725199.844485134\n",
      "batch [50] loss: 37725199.844485134\n",
      "batch [60] loss: 37725199.844485134\n",
      "batch [70] loss: 37725199.844485134\n",
      "batch [80] loss: 37725199.844485134\n",
      "batch [90] loss: 37725199.844485134\n",
      "batch [100] loss: 37725199.844485134\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 37725199.844485134\n",
      "batch [10] loss: 37725199.844485134\n",
      "batch [20] loss: 37725199.844485134\n",
      "batch [30] loss: 37725199.844485134\n",
      "batch [40] loss: 37725199.844485134\n",
      "batch [50] loss: 37725199.844485134\n",
      "batch [60] loss: 37725199.844485134\n",
      "batch [70] loss: 37725199.844485134\n",
      "batch [80] loss: 37725199.844485134\n",
      "batch [90] loss: 37725199.844485134\n",
      "batch [100] loss: 37725199.844485134\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 37725199.844485134\n",
      "batch [10] loss: 37725199.844485134\n",
      "batch [20] loss: 37725199.844485134\n",
      "batch [30] loss: 37725199.844485134\n",
      "batch [40] loss: 37725199.844485134\n",
      "batch [50] loss: 37725199.844485134\n",
      "batch [60] loss: 37725199.844485134\n",
      "batch [70] loss: 37725199.844485134\n",
      "batch [80] loss: 37725199.844485134\n",
      "batch [90] loss: 37725199.844485134\n",
      "batch [100] loss: 37725199.844485134\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 37725199.844485134\n",
      "batch [10] loss: 37725199.844485134\n",
      "batch [20] loss: 37725199.844485134\n",
      "batch [30] loss: 37725199.844485134\n",
      "batch [40] loss: 37725199.844485134\n",
      "batch [50] loss: 37725199.844485134\n",
      "batch [60] loss: 37725199.844485134\n",
      "batch [70] loss: 37725199.844485134\n",
      "batch [80] loss: 37725199.844485134\n",
      "batch [90] loss: 37725199.844485134\n",
      "batch [100] loss: 37725199.844485134\n",
      "OG Labels:  tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[8.4874e-02, 9.1513e-01],\n",
      "        [1.3805e-01, 8.6195e-01],\n",
      "        [1.4641e-01, 8.5359e-01],\n",
      "        [9.9995e-01, 4.9800e-05],\n",
      "        [1.0000e+00, 2.8401e-06],\n",
      "        [8.0803e-03, 9.9192e-01],\n",
      "        [1.0712e-01, 8.9288e-01],\n",
      "        [1.6469e-02, 9.8353e-01],\n",
      "        [1.3385e-01, 8.6615e-01],\n",
      "        [3.0658e-01, 6.9342e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.2355489520152805\n",
      "batch [10] loss: 30349303.51255984\n",
      "batch [20] loss: 30346823.13830152\n",
      "batch [30] loss: 30346701.727546867\n",
      "batch [40] loss: 30346711.95713464\n",
      "batch [50] loss: 30346703.242589623\n",
      "batch [60] loss: 30346768.763284713\n",
      "batch [70] loss: 30346897.293478865\n",
      "batch [80] loss: 30346801.707887463\n",
      "batch [90] loss: 30346825.452079818\n",
      "batch [100] loss: 30346801.697804265\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 30346800.927530654\n",
      "batch [10] loss: 30346800.785694554\n",
      "batch [20] loss: 30346800.785694554\n",
      "batch [30] loss: 30346800.785694554\n",
      "batch [40] loss: 30346800.785694588\n",
      "batch [50] loss: 30346800.785694554\n",
      "batch [60] loss: 30346800.785694554\n",
      "batch [70] loss: 30346800.785694554\n",
      "batch [80] loss: 30346800.785694554\n",
      "batch [90] loss: 30346800.785694554\n",
      "batch [100] loss: 30346800.785694554\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.07500000000000001, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 30346801.241091277\n",
      "batch [10] loss: 30346801.241091594\n",
      "batch [20] loss: 30346801.24109139\n",
      "batch [30] loss: 30346801.241091356\n",
      "batch [40] loss: 30346801.24109131\n",
      "batch [50] loss: 30346801.24109128\n",
      "batch [60] loss: 30346801.241091285\n",
      "batch [70] loss: 30346801.241919644\n",
      "batch [80] loss: 30346801.24109128\n",
      "batch [90] loss: 30346801.241091363\n",
      "batch [100] loss: 30346801.241091415\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 30346801.46878954\n",
      "batch [10] loss: 30346801.468797415\n",
      "batch [20] loss: 30346802.134528413\n",
      "batch [30] loss: 30346801.468790263\n",
      "batch [40] loss: 30346801.469332993\n",
      "batch [50] loss: 30346801.46878954\n",
      "batch [60] loss: 30346801.46879194\n",
      "batch [70] loss: 30346801.46878954\n",
      "batch [80] loss: 30346801.468789637\n",
      "batch [90] loss: 30346804.37036708\n",
      "batch [100] loss: 30346801.468791924\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.09375, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 30346801.58263874\n",
      "batch [10] loss: 30346801.582639184\n",
      "batch [20] loss: 30346801.58263875\n",
      "batch [30] loss: 30346801.582661748\n",
      "batch [40] loss: 30346802.595387388\n",
      "batch [50] loss: 30346801.582638796\n",
      "batch [60] loss: 30346801.582638737\n",
      "batch [70] loss: 30346801.58263945\n",
      "batch [80] loss: 30346801.58264648\n",
      "batch [90] loss: 30346801.582728088\n",
      "batch [100] loss: 30346802.07774226\n",
      "OG Labels:  tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[3.5310e-03, 9.9647e-01],\n",
      "        [8.3646e-03, 9.9164e-01],\n",
      "        [9.9682e-01, 3.1764e-03],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.7459e-02, 9.8254e-01],\n",
      "        [1.4896e-01, 8.5104e-01],\n",
      "        [4.2714e-02, 9.5729e-01],\n",
      "        [5.9767e-03, 9.9402e-01],\n",
      "        [1.3805e-01, 8.6195e-01],\n",
      "        [2.2446e-04, 9.9978e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.281587908120873\n",
      "batch [10] loss: 37439799.84566164\n",
      "batch [20] loss: 37439799.84566164\n",
      "batch [30] loss: 37439799.84566164\n",
      "batch [40] loss: 37439799.84566164\n",
      "batch [50] loss: 37439799.84566164\n",
      "batch [60] loss: 37439799.84566164\n",
      "batch [70] loss: 37439799.84566164\n",
      "batch [80] loss: 37439799.84566164\n",
      "batch [90] loss: 37439799.84566164\n",
      "batch [100] loss: 37439799.84566164\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 37439799.84566164\n",
      "batch [10] loss: 37439799.84566164\n",
      "batch [20] loss: 37439799.84566164\n",
      "batch [30] loss: 37439799.84566164\n",
      "batch [40] loss: 37439799.84566164\n",
      "batch [50] loss: 37439799.84566164\n",
      "batch [60] loss: 37439799.84566164\n",
      "batch [70] loss: 37439799.84566164\n",
      "batch [80] loss: 37439799.84566164\n",
      "batch [90] loss: 37439799.84566164\n",
      "batch [100] loss: 37439799.84566164\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 37439799.84566164\n",
      "batch [10] loss: 37439799.84566164\n",
      "batch [20] loss: 37439799.84566164\n",
      "batch [30] loss: 37439799.84566164\n",
      "batch [40] loss: 37439799.84566164\n",
      "batch [50] loss: 37439799.84566164\n",
      "batch [60] loss: 37439799.84566164\n",
      "batch [70] loss: 37439799.84566164\n",
      "batch [80] loss: 37439799.84566164\n",
      "batch [90] loss: 37439799.84566164\n",
      "batch [100] loss: 37439799.84566164\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 37439799.84566164\n",
      "batch [10] loss: 37439799.84566164\n",
      "batch [20] loss: 37439799.84566164\n",
      "batch [30] loss: 37439799.84566164\n",
      "batch [40] loss: 37439799.84566164\n",
      "batch [50] loss: 37439799.84566164\n",
      "batch [60] loss: 37439799.84566164\n",
      "batch [70] loss: 37439799.84566164\n",
      "batch [80] loss: 37439799.84566164\n",
      "batch [90] loss: 37439799.84566164\n",
      "batch [100] loss: 37439799.84566164\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 37439799.84566164\n",
      "batch [10] loss: 37439799.84566164\n",
      "batch [20] loss: 37439799.84566164\n",
      "batch [30] loss: 37439799.84566164\n",
      "batch [40] loss: 37439799.84566164\n",
      "batch [50] loss: 37439799.84566164\n",
      "batch [60] loss: 37439799.84566164\n",
      "batch [70] loss: 37439799.84566164\n",
      "batch [80] loss: 37439799.84566164\n",
      "batch [90] loss: 37439799.84566164\n",
      "batch [100] loss: 37439799.84566164\n",
      "OG Labels:  tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1])\n",
      "Model pred:  tensor([[8.2939e-02, 9.1706e-01],\n",
      "        [4.2065e-01, 5.7935e-01],\n",
      "        [9.9999e-01, 6.4921e-06],\n",
      "        [2.7624e-03, 9.9724e-01],\n",
      "        [1.0000e+00, 2.4634e-06],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.2379e-01, 8.7621e-01],\n",
      "        [8.2260e-02, 9.1774e-01],\n",
      "        [9.9843e-01, 1.5679e-03],\n",
      "        [7.9318e-02, 9.2068e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.916735315726552\n",
      "batch [10] loss: 25839437.08134959\n",
      "batch [20] loss: 25836172.471464135\n",
      "batch [30] loss: 25836201.2524146\n",
      "batch [40] loss: 25836237.422797866\n",
      "batch [50] loss: 25836102.04701151\n",
      "batch [60] loss: 25836101.98388711\n",
      "batch [70] loss: 25836105.430534907\n",
      "batch [80] loss: 25836101.92962131\n",
      "batch [90] loss: 25836102.01512257\n",
      "batch [100] loss: 25836203.91804973\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25836203.55673592\n",
      "batch [10] loss: 25836200.911429413\n",
      "batch [20] loss: 25836200.91142941\n",
      "batch [30] loss: 25836200.911429413\n",
      "batch [40] loss: 25836200.911429428\n",
      "batch [50] loss: 25836200.91142941\n",
      "batch [60] loss: 25836200.911429416\n",
      "batch [70] loss: 25836200.91142941\n",
      "batch [80] loss: 25836200.911429424\n",
      "batch [90] loss: 25836200.91142941\n",
      "batch [100] loss: 25836200.91142941\n",
      "Using scale consts: [0.025, 0.025, 0.07500000000000001, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 25836201.42039653\n",
      "batch [10] loss: 25836201.42039711\n",
      "batch [20] loss: 25836201.420397498\n",
      "batch [30] loss: 25836201.46201492\n",
      "batch [40] loss: 25836201.42039811\n",
      "batch [50] loss: 25836301.42028232\n",
      "batch [60] loss: 25836301.42028231\n",
      "batch [70] loss: 25836301.42028236\n",
      "batch [80] loss: 25836301.420283176\n",
      "batch [90] loss: 25836301.4202824\n",
      "batch [100] loss: 25836301.42028231\n",
      "Using scale consts: [0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 25836301.67474679\n",
      "batch [10] loss: 25836301.674746882\n",
      "batch [20] loss: 25836301.67661312\n",
      "batch [30] loss: 25836301.674747117\n",
      "batch [40] loss: 25836301.674774602\n",
      "batch [50] loss: 25836301.6747468\n",
      "batch [60] loss: 25836301.67474681\n",
      "batch [70] loss: 25836301.674746804\n",
      "batch [80] loss: 25836375.830824412\n",
      "batch [90] loss: 25836303.264867164\n",
      "batch [100] loss: 25836301.67474679\n",
      "Using scale consts: [0.00625, 0.00625, 0.09375, 0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 25836301.801979244\n",
      "batch [10] loss: 25836301.801979132\n",
      "batch [20] loss: 25836301.801979147\n",
      "batch [30] loss: 25836301.80470746\n",
      "batch [40] loss: 25836388.71964063\n",
      "batch [50] loss: 25836301.801982105\n",
      "batch [60] loss: 25836301.8032603\n",
      "batch [70] loss: 25836301.80201436\n",
      "batch [80] loss: 25836301.80204062\n",
      "batch [90] loss: 25836301.801979102\n",
      "batch [100] loss: 25836301.801979106\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[1.7613e-02, 9.8239e-01],\n",
      "        [1.0900e-01, 8.9100e-01],\n",
      "        [1.3805e-01, 8.6195e-01],\n",
      "        [1.7412e-02, 9.8259e-01],\n",
      "        [3.1997e-02, 9.6800e-01],\n",
      "        [1.0206e-01, 8.9794e-01],\n",
      "        [9.9970e-01, 3.0048e-04],\n",
      "        [1.8354e-02, 9.8165e-01],\n",
      "        [2.0320e-01, 7.9680e-01],\n",
      "        [3.0780e-03, 9.9692e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.680723273811528\n",
      "batch [10] loss: 33959499.86000851\n",
      "batch [20] loss: 33959499.86000851\n",
      "batch [30] loss: 33959499.86000851\n",
      "batch [40] loss: 33959499.86000851\n",
      "batch [50] loss: 33959499.86000851\n",
      "batch [60] loss: 33959499.86000851\n",
      "batch [70] loss: 33959499.86000851\n",
      "batch [80] loss: 33959499.86000851\n",
      "batch [90] loss: 33959499.86000851\n",
      "batch [100] loss: 33959499.86000851\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 33959499.86000851\n",
      "batch [10] loss: 33959499.86000851\n",
      "batch [20] loss: 33959499.86000851\n",
      "batch [30] loss: 33959499.86000851\n",
      "batch [40] loss: 33959499.86000851\n",
      "batch [50] loss: 33959499.86000851\n",
      "batch [60] loss: 33959499.86000851\n",
      "batch [70] loss: 33959499.86000851\n",
      "batch [80] loss: 33959499.86000851\n",
      "batch [90] loss: 33959499.86000851\n",
      "batch [100] loss: 33959499.86000851\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 33959499.86000851\n",
      "batch [10] loss: 33959499.86000851\n",
      "batch [20] loss: 33959499.86000851\n",
      "batch [30] loss: 33959499.86000851\n",
      "batch [40] loss: 33959499.86000851\n",
      "batch [50] loss: 33959499.86000851\n",
      "batch [60] loss: 33959499.86000851\n",
      "batch [70] loss: 33959499.86000851\n",
      "batch [80] loss: 33959499.86000851\n",
      "batch [90] loss: 33959499.86000851\n",
      "batch [100] loss: 33959499.86000851\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 33959499.86000851\n",
      "batch [10] loss: 33959499.86000851\n",
      "batch [20] loss: 33959499.86000851\n",
      "batch [30] loss: 33959499.86000851\n",
      "batch [40] loss: 33959499.86000851\n",
      "batch [50] loss: 33959499.86000851\n",
      "batch [60] loss: 33959499.86000851\n",
      "batch [70] loss: 33959499.86000851\n",
      "batch [80] loss: 33959499.86000851\n",
      "batch [90] loss: 33959499.86000851\n",
      "batch [100] loss: 33959499.86000851\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 33959499.86000851\n",
      "batch [10] loss: 33959499.86000851\n",
      "batch [20] loss: 33959499.86000851\n",
      "batch [30] loss: 33959499.86000851\n",
      "batch [40] loss: 33959499.86000851\n",
      "batch [50] loss: 33959499.86000851\n",
      "batch [60] loss: 33959499.86000851\n",
      "batch [70] loss: 33959499.86000851\n",
      "batch [80] loss: 33959499.86000851\n",
      "batch [90] loss: 33959499.86000851\n",
      "batch [100] loss: 33959499.86000851\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
      "Model pred:  tensor([[9.6240e-03, 9.9038e-01],\n",
      "        [1.5816e-01, 8.4184e-01],\n",
      "        [1.6636e-02, 9.8336e-01],\n",
      "        [1.3000e-01, 8.7000e-01],\n",
      "        [9.9998e-01, 1.5144e-05],\n",
      "        [8.4874e-02, 9.1513e-01],\n",
      "        [1.0000e+00, 4.3840e-06],\n",
      "        [9.9999e-01, 6.4314e-06],\n",
      "        [9.9996e-01, 4.1394e-05],\n",
      "        [6.0625e-03, 9.9394e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 6.5245689058463086\n",
      "batch [10] loss: 25144669.94337388\n",
      "batch [20] loss: 25117034.06030004\n",
      "batch [30] loss: 25113339.97771322\n",
      "batch [40] loss: 25113282.52630479\n",
      "batch [50] loss: 25113513.085117713\n",
      "batch [60] loss: 25113102.530288197\n",
      "batch [70] loss: 25113291.133433506\n",
      "batch [80] loss: 25112867.352364883\n",
      "batch [90] loss: 25113380.082340445\n",
      "batch [100] loss: 25112952.534758877\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25113012.6913922\n",
      "batch [10] loss: 25112601.59411119\n",
      "batch [20] loss: 25112601.594111197\n",
      "batch [30] loss: 25112601.59411506\n",
      "batch [40] loss: 25112601.606855467\n",
      "batch [50] loss: 25112601.5941136\n",
      "batch [60] loss: 25112601.59411925\n",
      "batch [70] loss: 25112601.594115954\n",
      "batch [80] loss: 25112601.59411217\n",
      "batch [90] loss: 25112601.605030138\n",
      "batch [100] loss: 25112601.59411122\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.07500000000000001, 0.07500000000000001, 0.07500000000000001, 0.025]\n",
      "batch [0] loss: 25112602.44292994\n",
      "batch [10] loss: 25112894.844803985\n",
      "batch [20] loss: 25112902.39545145\n",
      "batch [30] loss: 25112802.443904895\n",
      "batch [40] loss: 25112833.979000967\n",
      "batch [50] loss: 25112803.07036893\n",
      "batch [60] loss: 25112932.98881752\n",
      "batch [70] loss: 25112802.442872718\n",
      "batch [80] loss: 25112802.443469204\n",
      "batch [90] loss: 25112802.443242244\n",
      "batch [100] loss: 25112802.45271574\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.08750000000000001, 0.08750000000000001, 0.08750000000000001, 0.0125]\n",
      "batch [0] loss: 25112802.86831738\n",
      "batch [10] loss: 25112890.30724433\n",
      "batch [20] loss: 25113002.518768225\n",
      "batch [30] loss: 25112902.876759008\n",
      "batch [40] loss: 25112904.173677728\n",
      "batch [50] loss: 25112997.156580124\n",
      "batch [60] loss: 25113014.596973903\n",
      "batch [70] loss: 25113003.000436954\n",
      "batch [80] loss: 25113003.399942335\n",
      "batch [90] loss: 25113197.91496083\n",
      "batch [100] loss: 25113102.922070332\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.09375, 0.09375, 0.09375, 0.00625]\n",
      "batch [0] loss: 25113102.941308867\n",
      "batch [10] loss: 25113172.762400847\n",
      "batch [20] loss: 25113200.99445958\n",
      "batch [30] loss: 25113285.652095344\n",
      "batch [40] loss: 25113376.8817626\n",
      "batch [50] loss: 25113279.370118923\n",
      "batch [60] loss: 25113203.079335116\n",
      "batch [70] loss: 25113208.14337931\n",
      "batch [80] loss: 25113495.403051883\n",
      "batch [90] loss: 25113281.22390673\n",
      "batch [100] loss: 25113304.220049586\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0039, 0.9961],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.0070, 0.9930],\n",
      "        [0.0280, 0.9720],\n",
      "        [0.0530, 0.9470],\n",
      "        [0.0088, 0.9912],\n",
      "        [0.0460, 0.9540],\n",
      "        [0.3054, 0.6946],\n",
      "        [0.0111, 0.9889],\n",
      "        [0.1128, 0.8872]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.685788262880433\n",
      "batch [10] loss: 44787799.815370925\n",
      "batch [20] loss: 44787799.815370925\n",
      "batch [30] loss: 44787799.815370925\n",
      "batch [40] loss: 44787799.815370925\n",
      "batch [50] loss: 44787799.815370925\n",
      "batch [60] loss: 44787799.815370925\n",
      "batch [70] loss: 44787799.815370925\n",
      "batch [80] loss: 44787799.815370925\n",
      "batch [90] loss: 44787799.815370925\n",
      "batch [100] loss: 44787799.815370925\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 44787799.815370925\n",
      "batch [10] loss: 44787799.815370925\n",
      "batch [20] loss: 44787799.815370925\n",
      "batch [30] loss: 44787799.815370925\n",
      "batch [40] loss: 44787799.815370925\n",
      "batch [50] loss: 44787799.815370925\n",
      "batch [60] loss: 44787799.815370925\n",
      "batch [70] loss: 44787799.815370925\n",
      "batch [80] loss: 44787799.815370925\n",
      "batch [90] loss: 44787799.815370925\n",
      "batch [100] loss: 44787799.815370925\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 44787799.815370925\n",
      "batch [10] loss: 44787799.815370925\n",
      "batch [20] loss: 44787799.815370925\n",
      "batch [30] loss: 44787799.815370925\n",
      "batch [40] loss: 44787799.815370925\n",
      "batch [50] loss: 44787799.815370925\n",
      "batch [60] loss: 44787799.815370925\n",
      "batch [70] loss: 44787799.815370925\n",
      "batch [80] loss: 44787799.815370925\n",
      "batch [90] loss: 44787799.815370925\n",
      "batch [100] loss: 44787799.815370925\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 44787799.815370925\n",
      "batch [10] loss: 44787799.815370925\n",
      "batch [20] loss: 44787799.815370925\n",
      "batch [30] loss: 44787799.815370925\n",
      "batch [40] loss: 44787799.815370925\n",
      "batch [50] loss: 44787799.815370925\n",
      "batch [60] loss: 44787799.815370925\n",
      "batch [70] loss: 44787799.815370925\n",
      "batch [80] loss: 44787799.815370925\n",
      "batch [90] loss: 44787799.815370925\n",
      "batch [100] loss: 44787799.815370925\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 44787799.815370925\n",
      "batch [10] loss: 44787799.815370925\n",
      "batch [20] loss: 44787799.815370925\n",
      "batch [30] loss: 44787799.815370925\n",
      "batch [40] loss: 44787799.815370925\n",
      "batch [50] loss: 44787799.815370925\n",
      "batch [60] loss: 44787799.815370925\n",
      "batch [70] loss: 44787799.815370925\n",
      "batch [80] loss: 44787799.815370925\n",
      "batch [90] loss: 44787799.815370925\n",
      "batch [100] loss: 44787799.815370925\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
      "Model pred:  tensor([[3.8368e-05, 9.9996e-01],\n",
      "        [1.3721e-01, 8.6279e-01],\n",
      "        [1.1010e-02, 9.8899e-01],\n",
      "        [2.9848e-03, 9.9702e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [2.7089e-03, 9.9729e-01],\n",
      "        [1.3805e-01, 8.6195e-01],\n",
      "        [1.0000e+00, 1.1811e-06],\n",
      "        [9.9999e-01, 1.4160e-05],\n",
      "        [9.9971e-01, 2.8690e-04]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 6.544565899586539\n",
      "batch [10] loss: 32458997.238764342\n",
      "batch [20] loss: 32454379.221048873\n",
      "batch [30] loss: 32454270.596489884\n",
      "batch [40] loss: 32454225.093135267\n",
      "batch [50] loss: 32454201.89065416\n",
      "batch [60] loss: 32454279.27355688\n",
      "batch [70] loss: 32454202.112130184\n",
      "batch [80] loss: 32454361.086808205\n",
      "batch [90] loss: 32454264.897601206\n",
      "batch [100] loss: 32454201.867211938\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05]\n",
      "batch [0] loss: 32454219.556796655\n",
      "batch [10] loss: 32461293.089864504\n",
      "batch [20] loss: 32464585.242365807\n",
      "batch [30] loss: 32468446.36554257\n",
      "batch [40] loss: 32470961.946209364\n",
      "batch [50] loss: 32473601.664919615\n",
      "batch [60] loss: 32475220.68394602\n",
      "batch [70] loss: 32478161.34625926\n",
      "batch [80] loss: 32479991.053674065\n",
      "batch [90] loss: 32483093.126198106\n",
      "batch [100] loss: 32484389.796007186\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.07500000000000001, 0.025]\n",
      "batch [0] loss: 32484350.053570606\n",
      "batch [10] loss: 32562274.16752608\n",
      "batch [20] loss: 32636302.130277395\n",
      "batch [30] loss: 32710472.59609219\n",
      "batch [40] loss: 32800351.68409443\n",
      "batch [50] loss: 32952742.867995195\n",
      "batch [60] loss: 33165267.23641258\n",
      "batch [70] loss: 33418426.107349504\n",
      "batch [80] loss: 33673134.63667513\n",
      "batch [90] loss: 33915253.79883953\n",
      "batch [100] loss: 34148150.46201922\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 100.0, 0.08750000000000001, 0.0125]\n",
      "batch [0] loss: 34174775.19869268\n",
      "batch [10] loss: 35302339.970387354\n",
      "batch [20] loss: 36692139.88487014\n",
      "batch [30] loss: 37744910.552488096\n",
      "batch [40] loss: 38489529.72894338\n",
      "batch [50] loss: 39029973.85904151\n",
      "batch [60] loss: 39443124.511007175\n",
      "batch [70] loss: 39774914.676807724\n",
      "batch [80] loss: 40033949.9911535\n",
      "batch [90] loss: 40229531.10670234\n",
      "batch [100] loss: 40388150.487601735\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 1000.0, 0.09375, 0.00625]\n",
      "batch [0] loss: 40411222.50726394\n",
      "batch [10] loss: 40870771.439089164\n",
      "batch [20] loss: 41357497.225988135\n",
      "batch [30] loss: 41812936.845953345\n",
      "batch [40] loss: 42158292.5170247\n",
      "batch [50] loss: 42458601.24518353\n",
      "batch [60] loss: 42702399.73635729\n",
      "batch [70] loss: 42897024.77888754\n",
      "batch [80] loss: 43059828.68401636\n",
      "batch [90] loss: 43186419.91086624\n",
      "batch [100] loss: 43300135.50692561\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0117, 0.9883],\n",
      "        [0.0823, 0.9177],\n",
      "        [0.0574, 0.9426],\n",
      "        [0.1754, 0.8246],\n",
      "        [0.0145, 0.9855],\n",
      "        [0.1615, 0.8385],\n",
      "        [0.0587, 0.9413],\n",
      "        [0.0850, 0.9150],\n",
      "        [0.0823, 0.9177],\n",
      "        [0.1421, 0.8579]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.6420715135200554\n",
      "batch [10] loss: 38535899.841143176\n",
      "batch [20] loss: 38535899.841143176\n",
      "batch [30] loss: 38535899.841143176\n",
      "batch [40] loss: 38535899.841143176\n",
      "batch [50] loss: 38535899.841143176\n",
      "batch [60] loss: 38535899.841143176\n",
      "batch [70] loss: 38535899.841143176\n",
      "batch [80] loss: 38535899.841143176\n",
      "batch [90] loss: 38535899.841143176\n",
      "batch [100] loss: 38535899.841143176\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 38535899.841143176\n",
      "batch [10] loss: 38535899.841143176\n",
      "batch [20] loss: 38535899.841143176\n",
      "batch [30] loss: 38535899.841143176\n",
      "batch [40] loss: 38535899.841143176\n",
      "batch [50] loss: 38535899.841143176\n",
      "batch [60] loss: 38535899.841143176\n",
      "batch [70] loss: 38535899.841143176\n",
      "batch [80] loss: 38535899.841143176\n",
      "batch [90] loss: 38535899.841143176\n",
      "batch [100] loss: 38535899.841143176\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 38535899.841143176\n",
      "batch [10] loss: 38535899.841143176\n",
      "batch [20] loss: 38535899.841143176\n",
      "batch [30] loss: 38535899.841143176\n",
      "batch [40] loss: 38535899.841143176\n",
      "batch [50] loss: 38535899.841143176\n",
      "batch [60] loss: 38535899.841143176\n",
      "batch [70] loss: 38535899.841143176\n",
      "batch [80] loss: 38535899.841143176\n",
      "batch [90] loss: 38535899.841143176\n",
      "batch [100] loss: 38535899.841143176\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 38535899.841143176\n",
      "batch [10] loss: 38535899.841143176\n",
      "batch [20] loss: 38535899.841143176\n",
      "batch [30] loss: 38535899.841143176\n",
      "batch [40] loss: 38535899.841143176\n",
      "batch [50] loss: 38535899.841143176\n",
      "batch [60] loss: 38535899.841143176\n",
      "batch [70] loss: 38535899.841143176\n",
      "batch [80] loss: 38535899.841143176\n",
      "batch [90] loss: 38535899.841143176\n",
      "batch [100] loss: 38535899.841143176\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 38535899.841143176\n",
      "batch [10] loss: 38535899.841143176\n",
      "batch [20] loss: 38535899.841143176\n",
      "batch [30] loss: 38535899.841143176\n",
      "batch [40] loss: 38535899.841143176\n",
      "batch [50] loss: 38535899.841143176\n",
      "batch [60] loss: 38535899.841143176\n",
      "batch [70] loss: 38535899.841143176\n",
      "batch [80] loss: 38535899.841143176\n",
      "batch [90] loss: 38535899.841143176\n",
      "batch [100] loss: 38535899.841143176\n",
      "OG Labels:  tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[2.4682e-02, 9.7532e-01],\n",
      "        [1.0000e+00, 1.7148e-06],\n",
      "        [9.9973e-01, 2.7341e-04],\n",
      "        [3.3305e-02, 9.6670e-01],\n",
      "        [7.3150e-03, 9.9268e-01],\n",
      "        [2.0209e-03, 9.9798e-01],\n",
      "        [2.1299e-02, 9.7870e-01],\n",
      "        [3.0873e-03, 9.9691e-01],\n",
      "        [1.4178e-04, 9.9986e-01],\n",
      "        [1.2790e-01, 8.7210e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 6.002376508326607\n",
      "batch [10] loss: 38888574.084938586\n",
      "batch [20] loss: 38868540.471000746\n",
      "batch [30] loss: 38864915.71644287\n",
      "batch [40] loss: 38866092.010735355\n",
      "batch [50] loss: 38866385.54520702\n",
      "batch [60] loss: 38866828.576095894\n",
      "batch [70] loss: 38867557.249348685\n",
      "batch [80] loss: 38867715.04257932\n",
      "batch [90] loss: 38867707.09367902\n",
      "batch [100] loss: 38868523.76609448\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 38868562.93247848\n",
      "batch [10] loss: 38868000.54722132\n",
      "batch [20] loss: 38868000.53205011\n",
      "batch [30] loss: 38868194.64943608\n",
      "batch [40] loss: 38868100.22561988\n",
      "batch [50] loss: 38868100.4090291\n",
      "batch [60] loss: 38868100.51519653\n",
      "batch [70] loss: 38868000.52892731\n",
      "batch [80] loss: 38868020.239832684\n",
      "batch [90] loss: 38868000.526553296\n",
      "batch [100] loss: 38868000.89965467\n",
      "Using scale consts: [0.025, 0.07500000000000001, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 38868000.86994344\n",
      "batch [10] loss: 38868499.580990314\n",
      "batch [20] loss: 38868393.254304275\n",
      "batch [30] loss: 38868400.869888365\n",
      "batch [40] loss: 38868501.175315976\n",
      "batch [50] loss: 38868861.983802125\n",
      "batch [60] loss: 38868701.158418134\n",
      "batch [70] loss: 38868975.60612684\n",
      "batch [80] loss: 38868895.53285285\n",
      "batch [90] loss: 38868868.41686527\n",
      "batch [100] loss: 38868900.87154214\n",
      "Using scale consts: [0.0125, 0.08750000000000001, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 38869101.01997198\n",
      "batch [10] loss: 38869276.78584562\n",
      "batch [20] loss: 38869201.041067265\n",
      "batch [30] loss: 38869287.05265845\n",
      "batch [40] loss: 38869301.622857645\n",
      "batch [50] loss: 38869298.43704699\n",
      "batch [60] loss: 38869408.33843491\n",
      "batch [70] loss: 38869809.72867353\n",
      "batch [80] loss: 38869923.29480651\n",
      "batch [90] loss: 38870100.00656246\n",
      "batch [100] loss: 38869906.726718925\n",
      "Using scale consts: [0.00625, 0.09375, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 38869801.145789824\n",
      "batch [10] loss: 38870001.12635484\n",
      "batch [20] loss: 38870501.126490414\n",
      "batch [30] loss: 38870557.13123884\n",
      "batch [40] loss: 38870864.2497955\n",
      "batch [50] loss: 38870999.4130889\n",
      "batch [60] loss: 38871270.64032069\n",
      "batch [70] loss: 38871331.67060484\n",
      "batch [80] loss: 38871494.6587639\n",
      "batch [90] loss: 38871672.392719634\n",
      "batch [100] loss: 38871701.227799654\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1190, 0.8810],\n",
      "        [0.0575, 0.9425],\n",
      "        [0.1602, 0.8398],\n",
      "        [0.0850, 0.9150],\n",
      "        [0.0344, 0.9656],\n",
      "        [0.1146, 0.8854],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.5565, 0.4435],\n",
      "        [0.0112, 0.9888]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.375577871366149\n",
      "batch [10] loss: 31586799.86978951\n",
      "batch [20] loss: 31586799.86978951\n",
      "batch [30] loss: 31586799.86978951\n",
      "batch [40] loss: 31586799.86978951\n",
      "batch [50] loss: 31586799.86978951\n",
      "batch [60] loss: 31586799.86978951\n",
      "batch [70] loss: 31586799.86978951\n",
      "batch [80] loss: 31586799.86978951\n",
      "batch [90] loss: 31586799.86978951\n",
      "batch [100] loss: 31586799.86978951\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 31586799.86978951\n",
      "batch [10] loss: 31586799.86978951\n",
      "batch [20] loss: 31586799.86978951\n",
      "batch [30] loss: 31586799.86978951\n",
      "batch [40] loss: 31586799.86978951\n",
      "batch [50] loss: 31586799.86978951\n",
      "batch [60] loss: 31586799.86978951\n",
      "batch [70] loss: 31586799.86978951\n",
      "batch [80] loss: 31586799.86978951\n",
      "batch [90] loss: 31586799.86978951\n",
      "batch [100] loss: 31586799.86978951\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 31586799.86978951\n",
      "batch [10] loss: 31586799.86978951\n",
      "batch [20] loss: 31586799.86978951\n",
      "batch [30] loss: 31586799.86978951\n",
      "batch [40] loss: 31586799.86978951\n",
      "batch [50] loss: 31586799.86978951\n",
      "batch [60] loss: 31586799.86978951\n",
      "batch [70] loss: 31586799.86978951\n",
      "batch [80] loss: 31586799.86978951\n",
      "batch [90] loss: 31586799.86978951\n",
      "batch [100] loss: 31586799.86978951\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 31586799.86978951\n",
      "batch [10] loss: 31586799.86978951\n",
      "batch [20] loss: 31586799.86978951\n",
      "batch [30] loss: 31586799.86978951\n",
      "batch [40] loss: 31586799.86978951\n",
      "batch [50] loss: 31586799.86978951\n",
      "batch [60] loss: 31586799.86978951\n",
      "batch [70] loss: 31586799.86978951\n",
      "batch [80] loss: 31586799.86978951\n",
      "batch [90] loss: 31586799.86978951\n",
      "batch [100] loss: 31586799.86978951\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 31586799.86978951\n",
      "batch [10] loss: 31586799.86978951\n",
      "batch [20] loss: 31586799.86978951\n",
      "batch [30] loss: 31586799.86978951\n",
      "batch [40] loss: 31586799.86978951\n",
      "batch [50] loss: 31586799.86978951\n",
      "batch [60] loss: 31586799.86978951\n",
      "batch [70] loss: 31586799.86978951\n",
      "batch [80] loss: 31586799.86978951\n",
      "batch [90] loss: 31586799.86978951\n",
      "batch [100] loss: 31586799.86978951\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[1.0696e-01, 8.9304e-01],\n",
      "        [6.9915e-03, 9.9301e-01],\n",
      "        [2.3354e-02, 9.7665e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.7801e-03, 9.9822e-01],\n",
      "        [2.3138e-01, 7.6862e-01],\n",
      "        [9.9981e-01, 1.8618e-04],\n",
      "        [1.1009e-01, 8.8991e-01],\n",
      "        [9.8944e-03, 9.9011e-01],\n",
      "        [1.1899e-01, 8.8101e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.8047038158180415\n",
      "batch [10] loss: 39097399.838828504\n",
      "batch [20] loss: 39097399.838828504\n",
      "batch [30] loss: 39097399.838828504\n",
      "batch [40] loss: 39097399.838828504\n",
      "batch [50] loss: 39097399.838828504\n",
      "batch [60] loss: 39097399.838828504\n",
      "batch [70] loss: 39097399.838828504\n",
      "batch [80] loss: 39097399.838828504\n",
      "batch [90] loss: 39097399.838828504\n",
      "batch [100] loss: 39097399.838828504\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 39097399.838828504\n",
      "batch [10] loss: 39097399.838828504\n",
      "batch [20] loss: 39097399.838828504\n",
      "batch [30] loss: 39097399.838828504\n",
      "batch [40] loss: 39097399.838828504\n",
      "batch [50] loss: 39097399.838828504\n",
      "batch [60] loss: 39097399.838828504\n",
      "batch [70] loss: 39097399.838828504\n",
      "batch [80] loss: 39097399.838828504\n",
      "batch [90] loss: 39097399.838828504\n",
      "batch [100] loss: 39097399.838828504\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 39097399.838828504\n",
      "batch [10] loss: 39097399.838828504\n",
      "batch [20] loss: 39097399.838828504\n",
      "batch [30] loss: 39097399.838828504\n",
      "batch [40] loss: 39097399.838828504\n",
      "batch [50] loss: 39097399.838828504\n",
      "batch [60] loss: 39097399.838828504\n",
      "batch [70] loss: 39097399.838828504\n",
      "batch [80] loss: 39097399.838828504\n",
      "batch [90] loss: 39097399.838828504\n",
      "batch [100] loss: 39097399.838828504\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 39097399.838828504\n",
      "batch [10] loss: 39097399.838828504\n",
      "batch [20] loss: 39097399.838828504\n",
      "batch [30] loss: 39097399.838828504\n",
      "batch [40] loss: 39097399.838828504\n",
      "batch [50] loss: 39097399.838828504\n",
      "batch [60] loss: 39097399.838828504\n",
      "batch [70] loss: 39097399.838828504\n",
      "batch [80] loss: 39097399.838828504\n",
      "batch [90] loss: 39097399.838828504\n",
      "batch [100] loss: 39097399.838828504\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 39097399.838828504\n",
      "batch [10] loss: 39097399.838828504\n",
      "batch [20] loss: 39097399.838828504\n",
      "batch [30] loss: 39097399.838828504\n",
      "batch [40] loss: 39097399.838828504\n",
      "batch [50] loss: 39097399.838828504\n",
      "batch [60] loss: 39097399.838828504\n",
      "batch [70] loss: 39097399.838828504\n",
      "batch [80] loss: 39097399.838828504\n",
      "batch [90] loss: 39097399.838828504\n",
      "batch [100] loss: 39097399.838828504\n",
      "OG Labels:  tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0053, 0.9947],\n",
      "        [0.0106, 0.9894],\n",
      "        [0.9969, 0.0031],\n",
      "        [0.0808, 0.9192],\n",
      "        [0.1146, 0.8854],\n",
      "        [0.1602, 0.8398],\n",
      "        [0.0404, 0.9596],\n",
      "        [0.0963, 0.9037],\n",
      "        [0.0165, 0.9835],\n",
      "        [0.0298, 0.9702]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.467807731099373\n",
      "batch [10] loss: 33997099.859853506\n",
      "batch [20] loss: 33997099.859853506\n",
      "batch [30] loss: 33997099.859853506\n",
      "batch [40] loss: 33997099.859853506\n",
      "batch [50] loss: 33997099.859853506\n",
      "batch [60] loss: 33997099.859853506\n",
      "batch [70] loss: 33997099.859853506\n",
      "batch [80] loss: 33997099.859853506\n",
      "batch [90] loss: 33997099.859853506\n",
      "batch [100] loss: 33997099.859853506\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 33997099.859853506\n",
      "batch [10] loss: 33997099.859853506\n",
      "batch [20] loss: 33997099.859853506\n",
      "batch [30] loss: 33997099.859853506\n",
      "batch [40] loss: 33997099.859853506\n",
      "batch [50] loss: 33997099.859853506\n",
      "batch [60] loss: 33997099.859853506\n",
      "batch [70] loss: 33997099.859853506\n",
      "batch [80] loss: 33997099.859853506\n",
      "batch [90] loss: 33997099.859853506\n",
      "batch [100] loss: 33997099.859853506\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 33997099.859853506\n",
      "batch [10] loss: 33997099.859853506\n",
      "batch [20] loss: 33997099.859853506\n",
      "batch [30] loss: 33997099.859853506\n",
      "batch [40] loss: 33997099.859853506\n",
      "batch [50] loss: 33997099.859853506\n",
      "batch [60] loss: 33997099.859853506\n",
      "batch [70] loss: 33997099.859853506\n",
      "batch [80] loss: 33997099.859853506\n",
      "batch [90] loss: 33997099.859853506\n",
      "batch [100] loss: 33997099.859853506\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 33997099.859853506\n",
      "batch [10] loss: 33997099.859853506\n",
      "batch [20] loss: 33997099.859853506\n",
      "batch [30] loss: 33997099.859853506\n",
      "batch [40] loss: 33997099.859853506\n",
      "batch [50] loss: 33997099.859853506\n",
      "batch [60] loss: 33997099.859853506\n",
      "batch [70] loss: 33997099.859853506\n",
      "batch [80] loss: 33997099.859853506\n",
      "batch [90] loss: 33997099.859853506\n",
      "batch [100] loss: 33997099.859853506\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 33997099.859853506\n",
      "batch [10] loss: 33997099.859853506\n",
      "batch [20] loss: 33997099.859853506\n",
      "batch [30] loss: 33997099.859853506\n",
      "batch [40] loss: 33997099.859853506\n",
      "batch [50] loss: 33997099.859853506\n",
      "batch [60] loss: 33997099.859853506\n",
      "batch [70] loss: 33997099.859853506\n",
      "batch [80] loss: 33997099.859853506\n",
      "batch [90] loss: 33997099.859853506\n",
      "batch [100] loss: 33997099.859853506\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.0856e-02, 9.8914e-01],\n",
      "        [7.7026e-04, 9.9923e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.1285e-01, 8.8715e-01],\n",
      "        [1.6266e-01, 8.3734e-01],\n",
      "        [2.2716e-03, 9.9773e-01],\n",
      "        [2.8724e-02, 9.7128e-01],\n",
      "        [1.6019e-01, 8.3981e-01],\n",
      "        [1.1780e-02, 9.8822e-01],\n",
      "        [4.0273e-02, 9.5973e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.6663121414554\n",
      "batch [10] loss: 38626099.84077135\n",
      "batch [20] loss: 38626099.84077135\n",
      "batch [30] loss: 38626099.84077135\n",
      "batch [40] loss: 38626099.84077135\n",
      "batch [50] loss: 38626099.84077135\n",
      "batch [60] loss: 38626099.84077135\n",
      "batch [70] loss: 38626099.84077135\n",
      "batch [80] loss: 38626099.84077135\n",
      "batch [90] loss: 38626099.84077135\n",
      "batch [100] loss: 38626099.84077135\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 38626099.84077135\n",
      "batch [10] loss: 38626099.84077135\n",
      "batch [20] loss: 38626099.84077135\n",
      "batch [30] loss: 38626099.84077135\n",
      "batch [40] loss: 38626099.84077135\n",
      "batch [50] loss: 38626099.84077135\n",
      "batch [60] loss: 38626099.84077135\n",
      "batch [70] loss: 38626099.84077135\n",
      "batch [80] loss: 38626099.84077135\n",
      "batch [90] loss: 38626099.84077135\n",
      "batch [100] loss: 38626099.84077135\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 38626099.84077135\n",
      "batch [10] loss: 38626099.84077135\n",
      "batch [20] loss: 38626099.84077135\n",
      "batch [30] loss: 38626099.84077135\n",
      "batch [40] loss: 38626099.84077135\n",
      "batch [50] loss: 38626099.84077135\n",
      "batch [60] loss: 38626099.84077135\n",
      "batch [70] loss: 38626099.84077135\n",
      "batch [80] loss: 38626099.84077135\n",
      "batch [90] loss: 38626099.84077135\n",
      "batch [100] loss: 38626099.84077135\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 38626099.84077135\n",
      "batch [10] loss: 38626099.84077135\n",
      "batch [20] loss: 38626099.84077135\n",
      "batch [30] loss: 38626099.84077135\n",
      "batch [40] loss: 38626099.84077135\n",
      "batch [50] loss: 38626099.84077135\n",
      "batch [60] loss: 38626099.84077135\n",
      "batch [70] loss: 38626099.84077135\n",
      "batch [80] loss: 38626099.84077135\n",
      "batch [90] loss: 38626099.84077135\n",
      "batch [100] loss: 38626099.84077135\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 38626099.84077135\n",
      "batch [10] loss: 38626099.84077135\n",
      "batch [20] loss: 38626099.84077135\n",
      "batch [30] loss: 38626099.84077135\n",
      "batch [40] loss: 38626099.84077135\n",
      "batch [50] loss: 38626099.84077135\n",
      "batch [60] loss: 38626099.84077135\n",
      "batch [70] loss: 38626099.84077135\n",
      "batch [80] loss: 38626099.84077135\n",
      "batch [90] loss: 38626099.84077135\n",
      "batch [100] loss: 38626099.84077135\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[1.3819e-02, 9.8618e-01],\n",
      "        [9.3729e-02, 9.0627e-01],\n",
      "        [6.0554e-02, 9.3945e-01],\n",
      "        [8.3646e-03, 9.9164e-01],\n",
      "        [3.3389e-03, 9.9666e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.4138e-02, 9.8586e-01],\n",
      "        [1.9401e-02, 9.8060e-01],\n",
      "        [1.1285e-01, 8.8715e-01],\n",
      "        [9.9984e-01, 1.6257e-04]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.112558065407396\n",
      "batch [10] loss: 43802426.91341331\n",
      "batch [20] loss: 43780260.067700684\n",
      "batch [30] loss: 43778593.65831746\n",
      "batch [40] loss: 43778051.323993206\n",
      "batch [50] loss: 43778060.280629516\n",
      "batch [60] loss: 43778021.55639173\n",
      "batch [70] loss: 43778064.92988253\n",
      "batch [80] loss: 43779192.76821686\n",
      "batch [90] loss: 43778535.93084098\n",
      "batch [100] loss: 43779169.095030166\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 43778784.633741\n",
      "batch [10] loss: 43778199.85272531\n",
      "batch [20] loss: 43778199.82622476\n",
      "batch [30] loss: 43778199.830275685\n",
      "batch [40] loss: 43778199.8262237\n",
      "batch [50] loss: 43778199.82621974\n",
      "batch [60] loss: 43778199.82622002\n",
      "batch [70] loss: 43778199.82622273\n",
      "batch [80] loss: 43778199.82653776\n",
      "batch [90] loss: 43778199.83483487\n",
      "batch [100] loss: 43778199.82622662\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001]\n",
      "batch [0] loss: 43778299.82949738\n",
      "batch [10] loss: 43778399.41848728\n",
      "batch [20] loss: 43778339.122624025\n",
      "batch [30] loss: 43778685.243048415\n",
      "batch [40] loss: 43778500.15796348\n",
      "batch [50] loss: 43778697.711305305\n",
      "batch [60] loss: 43778799.74878022\n",
      "batch [70] loss: 43778500.04368308\n",
      "batch [80] loss: 43778499.829484746\n",
      "batch [90] loss: 43778835.82881652\n",
      "batch [100] loss: 43778692.72696115\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001]\n",
      "batch [0] loss: 43778599.83110638\n",
      "batch [10] loss: 43778881.33468741\n",
      "batch [20] loss: 43779175.3618778\n",
      "batch [30] loss: 43779458.4511839\n",
      "batch [40] loss: 43779319.20783691\n",
      "batch [50] loss: 43779303.1560968\n",
      "batch [60] loss: 43779596.03303084\n",
      "batch [70] loss: 43779680.978387356\n",
      "batch [80] loss: 43779700.160680704\n",
      "batch [90] loss: 43779935.0383212\n",
      "batch [100] loss: 43779799.20029546\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375]\n",
      "batch [0] loss: 43779699.83143115\n",
      "batch [10] loss: 43779899.8643187\n",
      "batch [20] loss: 43779699.832606986\n",
      "batch [30] loss: 43780092.29575969\n",
      "batch [40] loss: 43779799.858683944\n",
      "batch [50] loss: 43779887.97423069\n",
      "batch [60] loss: 43779963.913296446\n",
      "batch [70] loss: 43779881.14962175\n",
      "batch [80] loss: 43779899.83538534\n",
      "batch [90] loss: 43779950.59991366\n",
      "batch [100] loss: 43780097.9945759\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.1688e-02, 9.8831e-01],\n",
      "        [8.5506e-03, 9.9145e-01],\n",
      "        [1.6301e-01, 8.3699e-01],\n",
      "        [4.5845e-03, 9.9542e-01],\n",
      "        [1.0000e+00, 2.2723e-06],\n",
      "        [8.4906e-02, 9.1509e-01],\n",
      "        [9.3783e-02, 9.0622e-01],\n",
      "        [8.9411e-02, 9.1059e-01],\n",
      "        [8.3414e-02, 9.1659e-01],\n",
      "        [1.7005e-02, 9.8299e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.26223395331785\n",
      "batch [10] loss: 43012500.91915319\n",
      "batch [20] loss: 43012500.92851835\n",
      "batch [30] loss: 43012500.91913786\n",
      "batch [40] loss: 43012500.91913786\n",
      "batch [50] loss: 43012500.91960372\n",
      "batch [60] loss: 43012500.91913786\n",
      "batch [70] loss: 43012500.91913786\n",
      "batch [80] loss: 43012500.91913785\n",
      "batch [90] loss: 43012500.91913785\n",
      "batch [100] loss: 43012500.91913786\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 43012500.37091356\n",
      "batch [10] loss: 43012500.37091356\n",
      "batch [20] loss: 43012500.37091356\n",
      "batch [30] loss: 43012500.37091356\n",
      "batch [40] loss: 43012500.37091356\n",
      "batch [50] loss: 43012500.37091356\n",
      "batch [60] loss: 43012500.37091356\n",
      "batch [70] loss: 43012500.37091356\n",
      "batch [80] loss: 43012500.37091356\n",
      "batch [90] loss: 43012500.37091356\n",
      "batch [100] loss: 43012500.37091356\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 43012500.64502573\n",
      "batch [10] loss: 43012500.64502573\n",
      "batch [20] loss: 43012500.64502573\n",
      "batch [30] loss: 43012500.64502573\n",
      "batch [40] loss: 43012500.64502573\n",
      "batch [50] loss: 43012500.64502573\n",
      "batch [60] loss: 43012500.64502573\n",
      "batch [70] loss: 43012500.64502573\n",
      "batch [80] loss: 43012500.64502573\n",
      "batch [90] loss: 43012500.64502573\n",
      "batch [100] loss: 43012500.64502573\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 43012500.78208175\n",
      "batch [10] loss: 43012500.78208175\n",
      "batch [20] loss: 43012500.78208175\n",
      "batch [30] loss: 43012500.78208175\n",
      "batch [40] loss: 43012500.78208175\n",
      "batch [50] loss: 43012500.78208175\n",
      "batch [60] loss: 43012500.78208175\n",
      "batch [70] loss: 43012500.78208175\n",
      "batch [80] loss: 43012500.78208175\n",
      "batch [90] loss: 43012500.78208175\n",
      "batch [100] loss: 43012500.78208175\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 43012500.8506098\n",
      "batch [10] loss: 43012500.8506098\n",
      "batch [20] loss: 43012500.8506098\n",
      "batch [30] loss: 43012500.8506098\n",
      "batch [40] loss: 43012500.8506098\n",
      "batch [50] loss: 43012500.8506098\n",
      "batch [60] loss: 43012500.8506098\n",
      "batch [70] loss: 43012500.8506098\n",
      "batch [80] loss: 43012600.85056571\n",
      "batch [90] loss: 43012600.85056571\n",
      "batch [100] loss: 43012600.85056571\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.4376e-01, 8.5624e-01],\n",
      "        [1.7723e-02, 9.8228e-01],\n",
      "        [2.0548e-03, 9.9795e-01],\n",
      "        [8.2260e-02, 9.1774e-01],\n",
      "        [1.5353e-02, 9.8465e-01],\n",
      "        [9.9968e-01, 3.1701e-04],\n",
      "        [6.5089e-03, 9.9349e-01],\n",
      "        [3.9217e-02, 9.6078e-01],\n",
      "        [9.3783e-02, 9.0622e-01],\n",
      "        [1.2036e-02, 9.8796e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.151691220805734\n",
      "batch [10] loss: 38058999.84310911\n",
      "batch [20] loss: 38058999.84310911\n",
      "batch [30] loss: 38058999.84310911\n",
      "batch [40] loss: 38058999.84310911\n",
      "batch [50] loss: 38058999.84310911\n",
      "batch [60] loss: 38058999.84310911\n",
      "batch [70] loss: 38058999.84310911\n",
      "batch [80] loss: 38058999.84310911\n",
      "batch [90] loss: 38058999.84310911\n",
      "batch [100] loss: 38058999.84310911\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 38058999.84310911\n",
      "batch [10] loss: 38058999.84310911\n",
      "batch [20] loss: 38058999.84310911\n",
      "batch [30] loss: 38058999.84310911\n",
      "batch [40] loss: 38058999.84310911\n",
      "batch [50] loss: 38058999.84310911\n",
      "batch [60] loss: 38058999.84310911\n",
      "batch [70] loss: 38058999.84310911\n",
      "batch [80] loss: 38058999.84310911\n",
      "batch [90] loss: 38058999.84310911\n",
      "batch [100] loss: 38058999.84310911\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 38058999.84310911\n",
      "batch [10] loss: 38058999.84310911\n",
      "batch [20] loss: 38058999.84310911\n",
      "batch [30] loss: 38058999.84310911\n",
      "batch [40] loss: 38058999.84310911\n",
      "batch [50] loss: 38058999.84310911\n",
      "batch [60] loss: 38058999.84310911\n",
      "batch [70] loss: 38058999.84310911\n",
      "batch [80] loss: 38058999.84310911\n",
      "batch [90] loss: 38058999.84310911\n",
      "batch [100] loss: 38058999.84310911\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 38058999.84310911\n",
      "batch [10] loss: 38058999.84310911\n",
      "batch [20] loss: 38058999.84310911\n",
      "batch [30] loss: 38058999.84310911\n",
      "batch [40] loss: 38058999.84310911\n",
      "batch [50] loss: 38058999.84310911\n",
      "batch [60] loss: 38058999.84310911\n",
      "batch [70] loss: 38058999.84310911\n",
      "batch [80] loss: 38058999.84310911\n",
      "batch [90] loss: 38058999.84310911\n",
      "batch [100] loss: 38058999.84310911\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 38058999.84310911\n",
      "batch [10] loss: 38058999.84310911\n",
      "batch [20] loss: 38058999.84310911\n",
      "batch [30] loss: 38058999.84310911\n",
      "batch [40] loss: 38058999.84310911\n",
      "batch [50] loss: 38058999.84310911\n",
      "batch [60] loss: 38058999.84310911\n",
      "batch [70] loss: 38058999.84310911\n",
      "batch [80] loss: 38058999.84310911\n",
      "batch [90] loss: 38058999.84310911\n",
      "batch [100] loss: 38058999.84310911\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[4.7529e-02, 9.5247e-01],\n",
      "        [9.9395e-01, 6.0511e-03],\n",
      "        [1.7699e-01, 8.2301e-01],\n",
      "        [2.2716e-03, 9.9773e-01],\n",
      "        [1.7110e-02, 9.8289e-01],\n",
      "        [9.6431e-01, 3.5693e-02],\n",
      "        [1.2407e-01, 8.7593e-01],\n",
      "        [1.1435e-01, 8.8565e-01],\n",
      "        [1.6266e-01, 8.3734e-01],\n",
      "        [1.0000e+00, 4.6751e-07]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.328423435682015\n",
      "batch [10] loss: 35485199.8537191\n",
      "batch [20] loss: 35485199.8537191\n",
      "batch [30] loss: 35485199.8537191\n",
      "batch [40] loss: 35485199.8537191\n",
      "batch [50] loss: 35485199.8537191\n",
      "batch [60] loss: 35485199.8537191\n",
      "batch [70] loss: 35485199.8537191\n",
      "batch [80] loss: 35485199.8537191\n",
      "batch [90] loss: 35485199.8537191\n",
      "batch [100] loss: 35485199.8537191\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 35485199.8537191\n",
      "batch [10] loss: 35485199.8537191\n",
      "batch [20] loss: 35485199.8537191\n",
      "batch [30] loss: 35485199.8537191\n",
      "batch [40] loss: 35485199.8537191\n",
      "batch [50] loss: 35485199.8537191\n",
      "batch [60] loss: 35485199.8537191\n",
      "batch [70] loss: 35485199.8537191\n",
      "batch [80] loss: 35485199.8537191\n",
      "batch [90] loss: 35485199.8537191\n",
      "batch [100] loss: 35485199.8537191\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 35485199.8537191\n",
      "batch [10] loss: 35485199.8537191\n",
      "batch [20] loss: 35485199.8537191\n",
      "batch [30] loss: 35485199.8537191\n",
      "batch [40] loss: 35485199.8537191\n",
      "batch [50] loss: 35485199.8537191\n",
      "batch [60] loss: 35485199.8537191\n",
      "batch [70] loss: 35485199.8537191\n",
      "batch [80] loss: 35485199.8537191\n",
      "batch [90] loss: 35485199.8537191\n",
      "batch [100] loss: 35485199.8537191\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 35485199.8537191\n",
      "batch [10] loss: 35485199.8537191\n",
      "batch [20] loss: 35485199.8537191\n",
      "batch [30] loss: 35485199.8537191\n",
      "batch [40] loss: 35485199.8537191\n",
      "batch [50] loss: 35485199.8537191\n",
      "batch [60] loss: 35485199.8537191\n",
      "batch [70] loss: 35485199.8537191\n",
      "batch [80] loss: 35485199.8537191\n",
      "batch [90] loss: 35485199.8537191\n",
      "batch [100] loss: 35485199.8537191\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 35485199.8537191\n",
      "batch [10] loss: 35485199.8537191\n",
      "batch [20] loss: 35485199.8537191\n",
      "batch [30] loss: 35485199.8537191\n",
      "batch [40] loss: 35485199.8537191\n",
      "batch [50] loss: 35485199.8537191\n",
      "batch [60] loss: 35485199.8537191\n",
      "batch [70] loss: 35485199.8537191\n",
      "batch [80] loss: 35485199.8537191\n",
      "batch [90] loss: 35485199.8537191\n",
      "batch [100] loss: 35485199.8537191\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0])\n",
      "Model pred:  tensor([[6.8438e-03, 9.9316e-01],\n",
      "        [2.4430e-02, 9.7557e-01],\n",
      "        [3.1745e-03, 9.9683e-01],\n",
      "        [8.0803e-03, 9.9192e-01],\n",
      "        [7.8768e-03, 9.9212e-01],\n",
      "        [3.1065e-03, 9.9689e-01],\n",
      "        [9.9739e-01, 2.6076e-03],\n",
      "        [1.7866e-02, 9.8213e-01],\n",
      "        [1.7491e-02, 9.8251e-01],\n",
      "        [9.9997e-01, 2.8693e-05]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 5.427225837175481\n",
      "batch [10] loss: 41123200.855461076\n",
      "batch [20] loss: 41123200.700672776\n",
      "batch [30] loss: 41123200.717889324\n",
      "batch [40] loss: 41123200.70067142\n",
      "batch [50] loss: 41123200.70067103\n",
      "batch [60] loss: 41123200.70067103\n",
      "batch [70] loss: 41123200.70067105\n",
      "batch [80] loss: 41123200.70067103\n",
      "batch [90] loss: 41123300.70061687\n",
      "batch [100] loss: 41123400.700579084\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 41123400.2655279\n",
      "batch [10] loss: 41123400.2655279\n",
      "batch [20] loss: 41123400.2655279\n",
      "batch [30] loss: 41123400.2655279\n",
      "batch [40] loss: 41123400.2655279\n",
      "batch [50] loss: 41123400.2655279\n",
      "batch [60] loss: 41123400.2655279\n",
      "batch [70] loss: 41123400.2655279\n",
      "batch [80] loss: 41123400.2655279\n",
      "batch [90] loss: 41123400.2655279\n",
      "batch [100] loss: 41123400.2655279\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001]\n",
      "batch [0] loss: 41123400.483053505\n",
      "batch [10] loss: 41123400.483053505\n",
      "batch [20] loss: 41123400.483053505\n",
      "batch [30] loss: 41123400.483053505\n",
      "batch [40] loss: 41123400.483053505\n",
      "batch [50] loss: 41123400.483053505\n",
      "batch [60] loss: 41123400.483053505\n",
      "batch [70] loss: 41123400.483053505\n",
      "batch [80] loss: 41123400.483053505\n",
      "batch [90] loss: 41123400.483053505\n",
      "batch [100] loss: 41123400.483053505\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001]\n",
      "batch [0] loss: 41123400.59181626\n",
      "batch [10] loss: 41123400.59181626\n",
      "batch [20] loss: 41123400.59181626\n",
      "batch [30] loss: 41123400.59181626\n",
      "batch [40] loss: 41123400.59181626\n",
      "batch [50] loss: 41123400.59181626\n",
      "batch [60] loss: 41123400.59181626\n",
      "batch [70] loss: 41123400.59181626\n",
      "batch [80] loss: 41123400.59181626\n",
      "batch [90] loss: 41123400.591816336\n",
      "batch [100] loss: 41123400.59181626\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375]\n",
      "batch [0] loss: 41123400.64619768\n",
      "batch [10] loss: 41123400.64619768\n",
      "batch [20] loss: 41123400.64619768\n",
      "batch [30] loss: 41123400.64619768\n",
      "batch [40] loss: 41123400.64619768\n",
      "batch [50] loss: 41123400.64619768\n",
      "batch [60] loss: 41123400.64619768\n",
      "batch [70] loss: 41123400.64619768\n",
      "batch [80] loss: 41123400.64619768\n",
      "batch [90] loss: 41123400.64619768\n",
      "batch [100] loss: 41123400.64619768\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1128, 0.8872],\n",
      "        [0.0829, 0.9171],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.0069, 0.9931],\n",
      "        [0.1143, 0.8857],\n",
      "        [0.0073, 0.9927],\n",
      "        [0.1438, 0.8562],\n",
      "        [0.1118, 0.8882],\n",
      "        [0.0537, 0.9463],\n",
      "        [0.0411, 0.9589]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.1076016918796046\n",
      "batch [10] loss: 35843299.8522429\n",
      "batch [20] loss: 35843299.8522429\n",
      "batch [30] loss: 35843299.8522429\n",
      "batch [40] loss: 35843299.8522429\n",
      "batch [50] loss: 35843299.8522429\n",
      "batch [60] loss: 35843299.8522429\n",
      "batch [70] loss: 35843299.8522429\n",
      "batch [80] loss: 35843299.8522429\n",
      "batch [90] loss: 35843299.8522429\n",
      "batch [100] loss: 35843299.8522429\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 35843299.8522429\n",
      "batch [10] loss: 35843299.8522429\n",
      "batch [20] loss: 35843299.8522429\n",
      "batch [30] loss: 35843299.8522429\n",
      "batch [40] loss: 35843299.8522429\n",
      "batch [50] loss: 35843299.8522429\n",
      "batch [60] loss: 35843299.8522429\n",
      "batch [70] loss: 35843299.8522429\n",
      "batch [80] loss: 35843299.8522429\n",
      "batch [90] loss: 35843299.8522429\n",
      "batch [100] loss: 35843299.8522429\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 35843299.8522429\n",
      "batch [10] loss: 35843299.8522429\n",
      "batch [20] loss: 35843299.8522429\n",
      "batch [30] loss: 35843299.8522429\n",
      "batch [40] loss: 35843299.8522429\n",
      "batch [50] loss: 35843299.8522429\n",
      "batch [60] loss: 35843299.8522429\n",
      "batch [70] loss: 35843299.8522429\n",
      "batch [80] loss: 35843299.8522429\n",
      "batch [90] loss: 35843299.8522429\n",
      "batch [100] loss: 35843299.8522429\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 35843299.8522429\n",
      "batch [10] loss: 35843299.8522429\n",
      "batch [20] loss: 35843299.8522429\n",
      "batch [30] loss: 35843299.8522429\n",
      "batch [40] loss: 35843299.8522429\n",
      "batch [50] loss: 35843299.8522429\n",
      "batch [60] loss: 35843299.8522429\n",
      "batch [70] loss: 35843299.8522429\n",
      "batch [80] loss: 35843299.8522429\n",
      "batch [90] loss: 35843299.8522429\n",
      "batch [100] loss: 35843299.8522429\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 35843299.8522429\n",
      "batch [10] loss: 35843299.8522429\n",
      "batch [20] loss: 35843299.8522429\n",
      "batch [30] loss: 35843299.8522429\n",
      "batch [40] loss: 35843299.8522429\n",
      "batch [50] loss: 35843299.8522429\n",
      "batch [60] loss: 35843299.8522429\n",
      "batch [70] loss: 35843299.8522429\n",
      "batch [80] loss: 35843299.8522429\n",
      "batch [90] loss: 35843299.8522429\n",
      "batch [100] loss: 35843299.8522429\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[5.4783e-02, 9.4522e-01],\n",
      "        [1.1475e-01, 8.8525e-01],\n",
      "        [8.2939e-02, 9.1706e-01],\n",
      "        [3.2175e-03, 9.9678e-01],\n",
      "        [3.1674e-03, 9.9683e-01],\n",
      "        [3.2976e-03, 9.9670e-01],\n",
      "        [1.6301e-01, 8.3699e-01],\n",
      "        [8.8200e-03, 9.9118e-01],\n",
      "        [1.1688e-02, 9.8831e-01],\n",
      "        [5.2968e-04, 9.9947e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.2831010914278655\n",
      "batch [10] loss: 42038799.826703146\n",
      "batch [20] loss: 42038799.826703146\n",
      "batch [30] loss: 42038799.826703146\n",
      "batch [40] loss: 42038799.826703146\n",
      "batch [50] loss: 42038799.826703146\n",
      "batch [60] loss: 42038799.826703146\n",
      "batch [70] loss: 42038799.826703146\n",
      "batch [80] loss: 42038799.826703146\n",
      "batch [90] loss: 42038799.826703146\n",
      "batch [100] loss: 42038799.826703146\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 42038799.826703146\n",
      "batch [10] loss: 42038799.826703146\n",
      "batch [20] loss: 42038799.826703146\n",
      "batch [30] loss: 42038799.826703146\n",
      "batch [40] loss: 42038799.826703146\n",
      "batch [50] loss: 42038799.826703146\n",
      "batch [60] loss: 42038799.826703146\n",
      "batch [70] loss: 42038799.826703146\n",
      "batch [80] loss: 42038799.826703146\n",
      "batch [90] loss: 42038799.826703146\n",
      "batch [100] loss: 42038799.826703146\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 42038799.826703146\n",
      "batch [10] loss: 42038799.826703146\n",
      "batch [20] loss: 42038799.826703146\n",
      "batch [30] loss: 42038799.826703146\n",
      "batch [40] loss: 42038799.826703146\n",
      "batch [50] loss: 42038799.826703146\n",
      "batch [60] loss: 42038799.826703146\n",
      "batch [70] loss: 42038799.826703146\n",
      "batch [80] loss: 42038799.826703146\n",
      "batch [90] loss: 42038799.826703146\n",
      "batch [100] loss: 42038799.826703146\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 42038799.826703146\n",
      "batch [10] loss: 42038799.826703146\n",
      "batch [20] loss: 42038799.826703146\n",
      "batch [30] loss: 42038799.826703146\n",
      "batch [40] loss: 42038799.826703146\n",
      "batch [50] loss: 42038799.826703146\n",
      "batch [60] loss: 42038799.826703146\n",
      "batch [70] loss: 42038799.826703146\n",
      "batch [80] loss: 42038799.826703146\n",
      "batch [90] loss: 42038799.826703146\n",
      "batch [100] loss: 42038799.826703146\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 42038799.826703146\n",
      "batch [10] loss: 42038799.826703146\n",
      "batch [20] loss: 42038799.826703146\n",
      "batch [30] loss: 42038799.826703146\n",
      "batch [40] loss: 42038799.826703146\n",
      "batch [50] loss: 42038799.826703146\n",
      "batch [60] loss: 42038799.826703146\n",
      "batch [70] loss: 42038799.826703146\n",
      "batch [80] loss: 42038799.826703146\n",
      "batch [90] loss: 42038799.826703146\n",
      "batch [100] loss: 42038799.826703146\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
      "Model pred:  tensor([[0.1438, 0.8562],\n",
      "        [0.0903, 0.9097],\n",
      "        [0.0849, 0.9151],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.0427, 0.9573],\n",
      "        [0.0261, 0.9739],\n",
      "        [0.1143, 0.8857],\n",
      "        [0.9947, 0.0053],\n",
      "        [0.1214, 0.8786],\n",
      "        [0.0080, 0.9920]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.9702028871957005\n",
      "batch [10] loss: 29017699.880380128\n",
      "batch [20] loss: 29017699.880380128\n",
      "batch [30] loss: 29017699.880380128\n",
      "batch [40] loss: 29017699.880380128\n",
      "batch [50] loss: 29017699.880380128\n",
      "batch [60] loss: 29017699.880380128\n",
      "batch [70] loss: 29017699.880380128\n",
      "batch [80] loss: 29017699.880380128\n",
      "batch [90] loss: 29017699.880380128\n",
      "batch [100] loss: 29017699.880380128\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 29017699.880380128\n",
      "batch [10] loss: 29017699.880380128\n",
      "batch [20] loss: 29017699.880380128\n",
      "batch [30] loss: 29017699.880380128\n",
      "batch [40] loss: 29017699.880380128\n",
      "batch [50] loss: 29017699.880380128\n",
      "batch [60] loss: 29017699.880380128\n",
      "batch [70] loss: 29017699.880380128\n",
      "batch [80] loss: 29017699.880380128\n",
      "batch [90] loss: 29017699.880380128\n",
      "batch [100] loss: 29017699.880380128\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 29017699.880380128\n",
      "batch [10] loss: 29017699.880380128\n",
      "batch [20] loss: 29017699.880380128\n",
      "batch [30] loss: 29017699.880380128\n",
      "batch [40] loss: 29017699.880380128\n",
      "batch [50] loss: 29017699.880380128\n",
      "batch [60] loss: 29017699.880380128\n",
      "batch [70] loss: 29017699.880380128\n",
      "batch [80] loss: 29017699.880380128\n",
      "batch [90] loss: 29017699.880380128\n",
      "batch [100] loss: 29017699.880380128\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 29017699.880380128\n",
      "batch [10] loss: 29017699.880380128\n",
      "batch [20] loss: 29017699.880380128\n",
      "batch [30] loss: 29017699.880380128\n",
      "batch [40] loss: 29017699.880380128\n",
      "batch [50] loss: 29017699.880380128\n",
      "batch [60] loss: 29017699.880380128\n",
      "batch [70] loss: 29017699.880380128\n",
      "batch [80] loss: 29017699.880380128\n",
      "batch [90] loss: 29017699.880380128\n",
      "batch [100] loss: 29017699.880380128\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 29017699.880380128\n",
      "batch [10] loss: 29017699.880380128\n",
      "batch [20] loss: 29017699.880380128\n",
      "batch [30] loss: 29017699.880380128\n",
      "batch [40] loss: 29017699.880380128\n",
      "batch [50] loss: 29017699.880380128\n",
      "batch [60] loss: 29017699.880380128\n",
      "batch [70] loss: 29017699.880380128\n",
      "batch [80] loss: 29017699.880380128\n",
      "batch [90] loss: 29017699.880380128\n",
      "batch [100] loss: 29017699.880380128\n",
      "OG Labels:  tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[9.9914e-01, 8.6374e-04],\n",
      "        [1.4057e-01, 8.5943e-01],\n",
      "        [4.8744e-03, 9.9513e-01],\n",
      "        [9.9893e-01, 1.0704e-03],\n",
      "        [7.5891e-02, 9.2411e-01],\n",
      "        [9.3729e-02, 9.0627e-01],\n",
      "        [9.0265e-02, 9.0974e-01],\n",
      "        [8.2800e-02, 9.1720e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [7.3150e-03, 9.9268e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.7834992937849874\n",
      "batch [10] loss: 33294099.86275149\n",
      "batch [20] loss: 33294099.86275149\n",
      "batch [30] loss: 33294099.86275149\n",
      "batch [40] loss: 33294099.86275149\n",
      "batch [50] loss: 33294099.86275149\n",
      "batch [60] loss: 33294099.86275149\n",
      "batch [70] loss: 33294099.86275149\n",
      "batch [80] loss: 33294099.86275149\n",
      "batch [90] loss: 33294099.86275149\n",
      "batch [100] loss: 33294099.86275149\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 33294099.86275149\n",
      "batch [10] loss: 33294099.86275149\n",
      "batch [20] loss: 33294099.86275149\n",
      "batch [30] loss: 33294099.86275149\n",
      "batch [40] loss: 33294099.86275149\n",
      "batch [50] loss: 33294099.86275149\n",
      "batch [60] loss: 33294099.86275149\n",
      "batch [70] loss: 33294099.86275149\n",
      "batch [80] loss: 33294099.86275149\n",
      "batch [90] loss: 33294099.86275149\n",
      "batch [100] loss: 33294099.86275149\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 33294099.86275149\n",
      "batch [10] loss: 33294099.86275149\n",
      "batch [20] loss: 33294099.86275149\n",
      "batch [30] loss: 33294099.86275149\n",
      "batch [40] loss: 33294099.86275149\n",
      "batch [50] loss: 33294099.86275149\n",
      "batch [60] loss: 33294099.86275149\n",
      "batch [70] loss: 33294099.86275149\n",
      "batch [80] loss: 33294099.86275149\n",
      "batch [90] loss: 33294099.86275149\n",
      "batch [100] loss: 33294099.86275149\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 33294099.86275149\n",
      "batch [10] loss: 33294099.86275149\n",
      "batch [20] loss: 33294099.86275149\n",
      "batch [30] loss: 33294099.86275149\n",
      "batch [40] loss: 33294099.86275149\n",
      "batch [50] loss: 33294099.86275149\n",
      "batch [60] loss: 33294099.86275149\n",
      "batch [70] loss: 33294099.86275149\n",
      "batch [80] loss: 33294099.86275149\n",
      "batch [90] loss: 33294099.86275149\n",
      "batch [100] loss: 33294099.86275149\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 33294099.86275149\n",
      "batch [10] loss: 33294099.86275149\n",
      "batch [20] loss: 33294099.86275149\n",
      "batch [30] loss: 33294099.86275149\n",
      "batch [40] loss: 33294099.86275149\n",
      "batch [50] loss: 33294099.86275149\n",
      "batch [60] loss: 33294099.86275149\n",
      "batch [70] loss: 33294099.86275149\n",
      "batch [80] loss: 33294099.86275149\n",
      "batch [90] loss: 33294099.86275149\n",
      "batch [100] loss: 33294099.86275149\n",
      "OG Labels:  tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.7613e-02, 9.8239e-01],\n",
      "        [9.9999e-01, 9.9127e-06],\n",
      "        [9.9999e-01, 7.0910e-06],\n",
      "        [8.6246e-01, 1.3754e-01],\n",
      "        [1.4057e-01, 8.5943e-01],\n",
      "        [7.7086e-03, 9.9229e-01],\n",
      "        [2.1083e-01, 7.8917e-01],\n",
      "        [8.0803e-03, 9.9192e-01],\n",
      "        [1.1890e-01, 8.8110e-01],\n",
      "        [8.1712e-02, 9.1829e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.645623825287071\n",
      "batch [10] loss: 33395131.016187955\n",
      "batch [20] loss: 33392404.105262198\n",
      "batch [30] loss: 33391920.957853712\n",
      "batch [40] loss: 33392063.31722052\n",
      "batch [50] loss: 33392024.229039475\n",
      "batch [60] loss: 33392006.65127519\n",
      "batch [70] loss: 33391904.585957542\n",
      "batch [80] loss: 33392101.753804088\n",
      "batch [90] loss: 33392192.88365391\n",
      "batch [100] loss: 33392186.455610253\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 33392199.610059433\n",
      "batch [10] loss: 33392100.773269452\n",
      "batch [20] loss: 33392100.773269035\n",
      "batch [30] loss: 33392100.773269027\n",
      "batch [40] loss: 33392100.773269027\n",
      "batch [50] loss: 33392100.773269027\n",
      "batch [60] loss: 33392100.773269027\n",
      "batch [70] loss: 33392100.773269046\n",
      "batch [80] loss: 33392100.773269027\n",
      "batch [90] loss: 33392100.773269027\n",
      "batch [100] loss: 33392100.773269027\n",
      "Using scale consts: [0.025, 0.07500000000000001, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 33392101.22873005\n",
      "batch [10] loss: 33392101.228729874\n",
      "batch [20] loss: 33392101.228740044\n",
      "batch [30] loss: 33392101.228729893\n",
      "batch [40] loss: 33392101.228729874\n",
      "batch [50] loss: 33392101.22873889\n",
      "batch [60] loss: 33392101.228729848\n",
      "batch [70] loss: 33392101.228749193\n",
      "batch [80] loss: 33392101.228736352\n",
      "batch [90] loss: 33392101.228741284\n",
      "batch [100] loss: 33392101.22873589\n",
      "Using scale consts: [0.0125, 0.08750000000000001, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 33392101.45646012\n",
      "batch [10] loss: 33392101.45646179\n",
      "batch [20] loss: 33392101.456535503\n",
      "batch [30] loss: 33392101.46020566\n",
      "batch [40] loss: 33392101.456461523\n",
      "batch [50] loss: 33392200.91970256\n",
      "batch [60] loss: 33392101.456476007\n",
      "batch [70] loss: 33392101.457215216\n",
      "batch [80] loss: 33392101.45646012\n",
      "batch [90] loss: 33392101.456460178\n",
      "batch [100] loss: 33392101.48514626\n",
      "Using scale consts: [0.00625, 0.09375, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 33392101.570325777\n",
      "batch [10] loss: 33392101.57076428\n",
      "batch [20] loss: 33392101.874692738\n",
      "batch [30] loss: 33392201.568237\n",
      "batch [40] loss: 33392101.57037605\n",
      "batch [50] loss: 33392101.570325345\n",
      "batch [60] loss: 33392101.570327174\n",
      "batch [70] loss: 33392101.570325706\n",
      "batch [80] loss: 33392101.57032544\n",
      "batch [90] loss: 33392101.57092356\n",
      "batch [100] loss: 33392101.570325334\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[1.2595e-01, 8.7405e-01],\n",
      "        [1.1457e-01, 8.8543e-01],\n",
      "        [8.2260e-02, 9.1774e-01],\n",
      "        [1.4973e-02, 9.8503e-01],\n",
      "        [1.1435e-01, 8.8565e-01],\n",
      "        [8.4811e-02, 9.1519e-01],\n",
      "        [1.6019e-01, 8.3981e-01],\n",
      "        [1.4376e-01, 8.5624e-01],\n",
      "        [7.3150e-03, 9.9268e-01],\n",
      "        [9.9954e-01, 4.5553e-04]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.105163753173245\n",
      "batch [10] loss: 28231899.883619435\n",
      "batch [20] loss: 28231899.883619435\n",
      "batch [30] loss: 28231899.883619435\n",
      "batch [40] loss: 28231899.883619435\n",
      "batch [50] loss: 28231899.883619435\n",
      "batch [60] loss: 28231899.883619435\n",
      "batch [70] loss: 28231899.883619435\n",
      "batch [80] loss: 28231899.883619435\n",
      "batch [90] loss: 28231899.883619435\n",
      "batch [100] loss: 28231899.883619435\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 28231899.883619435\n",
      "batch [10] loss: 28231899.883619435\n",
      "batch [20] loss: 28231899.883619435\n",
      "batch [30] loss: 28231899.883619435\n",
      "batch [40] loss: 28231899.883619435\n",
      "batch [50] loss: 28231899.883619435\n",
      "batch [60] loss: 28231899.883619435\n",
      "batch [70] loss: 28231899.883619435\n",
      "batch [80] loss: 28231899.883619435\n",
      "batch [90] loss: 28231899.883619435\n",
      "batch [100] loss: 28231899.883619435\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 28231899.883619435\n",
      "batch [10] loss: 28231899.883619435\n",
      "batch [20] loss: 28231899.883619435\n",
      "batch [30] loss: 28231899.883619435\n",
      "batch [40] loss: 28231899.883619435\n",
      "batch [50] loss: 28231899.883619435\n",
      "batch [60] loss: 28231899.883619435\n",
      "batch [70] loss: 28231899.883619435\n",
      "batch [80] loss: 28231899.883619435\n",
      "batch [90] loss: 28231899.883619435\n",
      "batch [100] loss: 28231899.883619435\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 28231899.883619435\n",
      "batch [10] loss: 28231899.883619435\n",
      "batch [20] loss: 28231899.883619435\n",
      "batch [30] loss: 28231899.883619435\n",
      "batch [40] loss: 28231899.883619435\n",
      "batch [50] loss: 28231899.883619435\n",
      "batch [60] loss: 28231899.883619435\n",
      "batch [70] loss: 28231899.883619435\n",
      "batch [80] loss: 28231899.883619435\n",
      "batch [90] loss: 28231899.883619435\n",
      "batch [100] loss: 28231899.883619435\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 28231899.883619435\n",
      "batch [10] loss: 28231899.883619435\n",
      "batch [20] loss: 28231899.883619435\n",
      "batch [30] loss: 28231899.883619435\n",
      "batch [40] loss: 28231899.883619435\n",
      "batch [50] loss: 28231899.883619435\n",
      "batch [60] loss: 28231899.883619435\n",
      "batch [70] loss: 28231899.883619435\n",
      "batch [80] loss: 28231899.883619435\n",
      "batch [90] loss: 28231899.883619435\n",
      "batch [100] loss: 28231899.883619435\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.4655e-03, 9.9853e-01],\n",
      "        [1.1435e-01, 8.8565e-01],\n",
      "        [2.6148e-02, 9.7385e-01],\n",
      "        [1.2373e-01, 8.7627e-01],\n",
      "        [3.5629e-01, 6.4371e-01],\n",
      "        [1.6301e-01, 8.3699e-01],\n",
      "        [1.7762e-01, 8.2238e-01],\n",
      "        [5.0880e-04, 9.9949e-01],\n",
      "        [1.6266e-01, 8.3734e-01],\n",
      "        [1.7331e-01, 8.2669e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.8690117596070746\n",
      "batch [10] loss: 35036799.85556755\n",
      "batch [20] loss: 35036799.85556755\n",
      "batch [30] loss: 35036799.85556755\n",
      "batch [40] loss: 35036799.85556755\n",
      "batch [50] loss: 35036799.85556755\n",
      "batch [60] loss: 35036799.85556755\n",
      "batch [70] loss: 35036799.85556755\n",
      "batch [80] loss: 35036799.85556755\n",
      "batch [90] loss: 35036799.85556755\n",
      "batch [100] loss: 35036799.85556755\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 35036799.85556755\n",
      "batch [10] loss: 35036799.85556755\n",
      "batch [20] loss: 35036799.85556755\n",
      "batch [30] loss: 35036799.85556755\n",
      "batch [40] loss: 35036799.85556755\n",
      "batch [50] loss: 35036799.85556755\n",
      "batch [60] loss: 35036799.85556755\n",
      "batch [70] loss: 35036799.85556755\n",
      "batch [80] loss: 35036799.85556755\n",
      "batch [90] loss: 35036799.85556755\n",
      "batch [100] loss: 35036799.85556755\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 35036799.85556755\n",
      "batch [10] loss: 35036799.85556755\n",
      "batch [20] loss: 35036799.85556755\n",
      "batch [30] loss: 35036799.85556755\n",
      "batch [40] loss: 35036799.85556755\n",
      "batch [50] loss: 35036799.85556755\n",
      "batch [60] loss: 35036799.85556755\n",
      "batch [70] loss: 35036799.85556755\n",
      "batch [80] loss: 35036799.85556755\n",
      "batch [90] loss: 35036799.85556755\n",
      "batch [100] loss: 35036799.85556755\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 35036799.85556755\n",
      "batch [10] loss: 35036799.85556755\n",
      "batch [20] loss: 35036799.85556755\n",
      "batch [30] loss: 35036799.85556755\n",
      "batch [40] loss: 35036799.85556755\n",
      "batch [50] loss: 35036799.85556755\n",
      "batch [60] loss: 35036799.85556755\n",
      "batch [70] loss: 35036799.85556755\n",
      "batch [80] loss: 35036799.85556755\n",
      "batch [90] loss: 35036799.85556755\n",
      "batch [100] loss: 35036799.85556755\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 35036799.85556755\n",
      "batch [10] loss: 35036799.85556755\n",
      "batch [20] loss: 35036799.85556755\n",
      "batch [30] loss: 35036799.85556755\n",
      "batch [40] loss: 35036799.85556755\n",
      "batch [50] loss: 35036799.85556755\n",
      "batch [60] loss: 35036799.85556755\n",
      "batch [70] loss: 35036799.85556755\n",
      "batch [80] loss: 35036799.85556755\n",
      "batch [90] loss: 35036799.85556755\n",
      "batch [100] loss: 35036799.85556755\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[4.5774e-03, 9.9542e-01],\n",
      "        [6.3369e-03, 9.9366e-01],\n",
      "        [2.5863e-01, 7.4137e-01],\n",
      "        [2.2716e-03, 9.9773e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [9.9989e-01, 1.1148e-04],\n",
      "        [3.9997e-03, 9.9600e-01],\n",
      "        [2.3594e-02, 9.7641e-01],\n",
      "        [1.6019e-01, 8.3981e-01],\n",
      "        [9.9412e-01, 5.8819e-03]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.512318123106764\n",
      "batch [10] loss: 38320768.91678814\n",
      "batch [20] loss: 38302169.80602038\n",
      "batch [30] loss: 38300660.41415499\n",
      "batch [40] loss: 38300590.48057644\n",
      "batch [50] loss: 38300589.245238885\n",
      "batch [60] loss: 38301306.30450517\n",
      "batch [70] loss: 38301619.1905527\n",
      "batch [80] loss: 38301507.72110332\n",
      "batch [90] loss: 38302401.28539663\n",
      "batch [100] loss: 38302742.01911962\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 38302696.89240934\n",
      "batch [10] loss: 38302105.841486305\n",
      "batch [20] loss: 38302099.944595896\n",
      "batch [30] loss: 38302099.94986631\n",
      "batch [40] loss: 38302099.94443177\n",
      "batch [50] loss: 38302099.94443178\n",
      "batch [60] loss: 38302099.94443183\n",
      "batch [70] loss: 38302099.944431745\n",
      "batch [80] loss: 38302099.94445669\n",
      "batch [90] loss: 38302099.94443222\n",
      "batch [100] loss: 38302100.115298845\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 38302099.99576473\n",
      "batch [10] loss: 38302263.46608657\n",
      "batch [20] loss: 38302300.14390625\n",
      "batch [30] loss: 38302299.99558222\n",
      "batch [40] loss: 38302597.23696502\n",
      "batch [50] loss: 38302465.99079145\n",
      "batch [60] loss: 38302851.67797619\n",
      "batch [70] loss: 38302699.99708035\n",
      "batch [80] loss: 38302800.1248\n",
      "batch [90] loss: 38302800.39789137\n",
      "batch [100] loss: 38303056.48517248\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 38302801.074148744\n",
      "batch [10] loss: 38302800.02903722\n",
      "batch [20] loss: 38302900.36385414\n",
      "batch [30] loss: 38303095.13708734\n",
      "batch [40] loss: 38303094.80669342\n",
      "batch [50] loss: 38303300.02030179\n",
      "batch [60] loss: 38303203.97239593\n",
      "batch [70] loss: 38303300.08547163\n",
      "batch [80] loss: 38303306.8179312\n",
      "batch [90] loss: 38303412.01830658\n",
      "batch [100] loss: 38303652.959638044\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 38303408.42354337\n",
      "batch [10] loss: 38303404.53867519\n",
      "batch [20] loss: 38303728.817687586\n",
      "batch [30] loss: 38303600.02955751\n",
      "batch [40] loss: 38303599.18093473\n",
      "batch [50] loss: 38303885.47246228\n",
      "batch [60] loss: 38303900.12191933\n",
      "batch [70] loss: 38304108.87684645\n",
      "batch [80] loss: 38304249.06058413\n",
      "batch [90] loss: 38304209.153946295\n",
      "batch [100] loss: 38304215.49441945\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.1285e-01, 8.8715e-01],\n",
      "        [4.8481e-03, 9.9515e-01],\n",
      "        [8.0787e-02, 9.1921e-01],\n",
      "        [1.1285e-01, 8.8715e-01],\n",
      "        [1.7548e-02, 9.8245e-01],\n",
      "        [9.9991e-01, 9.4504e-05],\n",
      "        [8.2049e-03, 9.9180e-01],\n",
      "        [8.1712e-02, 9.1829e-01],\n",
      "        [1.4133e-02, 9.8587e-01],\n",
      "        [5.7178e-04, 9.9943e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.409715673945205\n",
      "batch [10] loss: 29955783.220352903\n",
      "batch [20] loss: 29932457.15095055\n",
      "batch [30] loss: 29931804.73192332\n",
      "batch [40] loss: 29930542.779359456\n",
      "batch [50] loss: 29930157.562463097\n",
      "batch [60] loss: 29930842.658868458\n",
      "batch [70] loss: 29930201.56093761\n",
      "batch [80] loss: 29930526.836852\n",
      "batch [90] loss: 29930749.565799817\n",
      "batch [100] loss: 29931070.62078268\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 29930907.0954924\n",
      "batch [10] loss: 29930536.86545649\n",
      "batch [20] loss: 29930500.056119192\n",
      "batch [30] loss: 29930600.148804963\n",
      "batch [40] loss: 29930600.057175398\n",
      "batch [50] loss: 29930600.11719523\n",
      "batch [60] loss: 29930600.055734716\n",
      "batch [70] loss: 29930600.05573611\n",
      "batch [80] loss: 29930600.055736344\n",
      "batch [90] loss: 29930600.074538805\n",
      "batch [100] loss: 29930600.74155356\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 29930797.603076953\n",
      "batch [10] loss: 29930600.485474207\n",
      "batch [20] loss: 29930600.14710584\n",
      "batch [30] loss: 29930700.15452632\n",
      "batch [40] loss: 29930791.96765772\n",
      "batch [50] loss: 29930625.79109205\n",
      "batch [60] loss: 29930695.27932144\n",
      "batch [70] loss: 29930600.145300664\n",
      "batch [80] loss: 29930700.11601886\n",
      "batch [90] loss: 29930600.146716714\n",
      "batch [100] loss: 29930794.109665323\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.08750000000000001, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 29930884.328354523\n",
      "batch [10] loss: 29930864.245840497\n",
      "batch [20] loss: 29930800.26452806\n",
      "batch [30] loss: 29930800.232161343\n",
      "batch [40] loss: 29930799.233924314\n",
      "batch [50] loss: 29930797.452725768\n",
      "batch [60] loss: 29930700.19082656\n",
      "batch [70] loss: 29930700.19325082\n",
      "batch [80] loss: 29931141.74235989\n",
      "batch [90] loss: 29931057.646777257\n",
      "batch [100] loss: 29931000.190001316\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.09375, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 29931000.22141871\n",
      "batch [10] loss: 29931093.39361062\n",
      "batch [20] loss: 29931000.212498404\n",
      "batch [30] loss: 29931120.04989659\n",
      "batch [40] loss: 29931269.718440674\n",
      "batch [50] loss: 29931202.02311907\n",
      "batch [60] loss: 29931203.14598766\n",
      "batch [70] loss: 29931200.67918991\n",
      "batch [80] loss: 29931202.50032178\n",
      "batch [90] loss: 29931400.185439475\n",
      "batch [100] loss: 29931593.11092654\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.1457e-01, 8.8543e-01],\n",
      "        [1.3721e-01, 8.6279e-01],\n",
      "        [1.1117e-03, 9.9889e-01],\n",
      "        [2.2315e-04, 9.9978e-01],\n",
      "        [1.2051e-02, 9.8795e-01],\n",
      "        [1.0000e+00, 1.4816e-06],\n",
      "        [7.3150e-03, 9.9268e-01],\n",
      "        [1.4360e-01, 8.5640e-01],\n",
      "        [1.1435e-01, 8.8565e-01],\n",
      "        [8.1712e-02, 9.1829e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.808312698345166\n",
      "batch [10] loss: 32936401.040864576\n",
      "batch [20] loss: 32936401.038593993\n",
      "batch [30] loss: 32936401.038593512\n",
      "batch [40] loss: 32936401.03859341\n",
      "batch [50] loss: 32936401.03859342\n",
      "batch [60] loss: 32936401.03859341\n",
      "batch [70] loss: 32936401.03859341\n",
      "batch [80] loss: 32936401.03859341\n",
      "batch [90] loss: 32936401.03859341\n",
      "batch [100] loss: 32936401.038593583\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 32936411.607899603\n",
      "batch [10] loss: 32948556.18819169\n",
      "batch [20] loss: 32956483.173789203\n",
      "batch [30] loss: 32962555.600117695\n",
      "batch [40] loss: 32967595.217106774\n",
      "batch [50] loss: 32972137.77621878\n",
      "batch [60] loss: 32976600.12229403\n",
      "batch [70] loss: 32981081.610034075\n",
      "batch [80] loss: 32983858.97376719\n",
      "batch [90] loss: 32986708.364479713\n",
      "batch [100] loss: 32988480.430934694\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 32989161.450818308\n",
      "batch [10] loss: 33088179.701166194\n",
      "batch [20] loss: 33179659.963333637\n",
      "batch [30] loss: 33264254.35084807\n",
      "batch [40] loss: 33409507.458764646\n",
      "batch [50] loss: 33616607.44984123\n",
      "batch [60] loss: 33840263.59126868\n",
      "batch [70] loss: 34025182.99446106\n",
      "batch [80] loss: 34183261.371518075\n",
      "batch [90] loss: 34321375.95019423\n",
      "batch [100] loss: 34436528.13940605\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 100.0, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 34447771.190080695\n",
      "batch [10] loss: 34914126.434701964\n",
      "batch [20] loss: 35614802.52409995\n",
      "batch [30] loss: 36377220.847936004\n",
      "batch [40] loss: 37142180.541930616\n",
      "batch [50] loss: 37836498.84237012\n",
      "batch [60] loss: 38413903.36274105\n",
      "batch [70] loss: 38875705.37238823\n",
      "batch [80] loss: 39268204.67055011\n",
      "batch [90] loss: 39597351.05357899\n",
      "batch [100] loss: 39888975.76995413\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 1000.0, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 39922520.77363594\n",
      "batch [10] loss: 40730956.989969835\n",
      "batch [20] loss: 41427480.88160575\n",
      "batch [30] loss: 41935199.39791773\n",
      "batch [40] loss: 42307094.36308969\n",
      "batch [50] loss: 42556048.154427\n",
      "batch [60] loss: 42759945.96327812\n",
      "batch [70] loss: 42906024.42201211\n",
      "batch [80] loss: 43071192.03239401\n",
      "batch [90] loss: 43206996.14656871\n",
      "batch [100] loss: 43331149.88210927\n",
      "OG Labels:  tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[9.9910e-01, 9.0080e-04],\n",
      "        [4.2714e-02, 9.5729e-01],\n",
      "        [1.1231e-02, 9.8877e-01],\n",
      "        [1.4359e-01, 8.5641e-01],\n",
      "        [9.9837e-01, 1.6284e-03],\n",
      "        [1.0583e-02, 9.8942e-01],\n",
      "        [8.3646e-03, 9.9164e-01],\n",
      "        [2.0602e-03, 9.9794e-01],\n",
      "        [1.4359e-01, 8.5641e-01],\n",
      "        [3.4265e-02, 9.6573e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.342365436889649\n",
      "batch [10] loss: 42668199.82410857\n",
      "batch [20] loss: 42668199.82410857\n",
      "batch [30] loss: 42668199.82410857\n",
      "batch [40] loss: 42668199.82410857\n",
      "batch [50] loss: 42668199.82410857\n",
      "batch [60] loss: 42668199.82410857\n",
      "batch [70] loss: 42668199.82410857\n",
      "batch [80] loss: 42668199.82410857\n",
      "batch [90] loss: 42668199.82410857\n",
      "batch [100] loss: 42668199.82410857\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 42668199.82410857\n",
      "batch [10] loss: 42668199.82410857\n",
      "batch [20] loss: 42668199.82410857\n",
      "batch [30] loss: 42668199.82410857\n",
      "batch [40] loss: 42668199.82410857\n",
      "batch [50] loss: 42668199.82410857\n",
      "batch [60] loss: 42668199.82410857\n",
      "batch [70] loss: 42668199.82410857\n",
      "batch [80] loss: 42668199.82410857\n",
      "batch [90] loss: 42668199.82410857\n",
      "batch [100] loss: 42668199.82410857\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 42668199.82410857\n",
      "batch [10] loss: 42668199.82410857\n",
      "batch [20] loss: 42668199.82410857\n",
      "batch [30] loss: 42668199.82410857\n",
      "batch [40] loss: 42668199.82410857\n",
      "batch [50] loss: 42668199.82410857\n",
      "batch [60] loss: 42668199.82410857\n",
      "batch [70] loss: 42668199.82410857\n",
      "batch [80] loss: 42668199.82410857\n",
      "batch [90] loss: 42668199.82410857\n",
      "batch [100] loss: 42668199.82410857\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 42668199.82410857\n",
      "batch [10] loss: 42668199.82410857\n",
      "batch [20] loss: 42668199.82410857\n",
      "batch [30] loss: 42668199.82410857\n",
      "batch [40] loss: 42668199.82410857\n",
      "batch [50] loss: 42668199.82410857\n",
      "batch [60] loss: 42668199.82410857\n",
      "batch [70] loss: 42668199.82410857\n",
      "batch [80] loss: 42668199.82410857\n",
      "batch [90] loss: 42668199.82410857\n",
      "batch [100] loss: 42668199.82410857\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 42668199.82410857\n",
      "batch [10] loss: 42668199.82410857\n",
      "batch [20] loss: 42668199.82410857\n",
      "batch [30] loss: 42668199.82410857\n",
      "batch [40] loss: 42668199.82410857\n",
      "batch [50] loss: 42668199.82410857\n",
      "batch [60] loss: 42668199.82410857\n",
      "batch [70] loss: 42668199.82410857\n",
      "batch [80] loss: 42668199.82410857\n",
      "batch [90] loss: 42668199.82410857\n",
      "batch [100] loss: 42668199.82410857\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0017, 0.9983],\n",
      "        [0.3185, 0.6815],\n",
      "        [0.0066, 0.9934],\n",
      "        [0.0164, 0.9836],\n",
      "        [0.1128, 0.8872],\n",
      "        [0.1381, 0.8619],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.1615, 0.8385],\n",
      "        [0.0575, 0.9425],\n",
      "        [0.0742, 0.9258]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.9500367192715684\n",
      "batch [10] loss: 29525499.87828682\n",
      "batch [20] loss: 29525499.87828682\n",
      "batch [30] loss: 29525499.87828682\n",
      "batch [40] loss: 29525499.87828682\n",
      "batch [50] loss: 29525499.87828682\n",
      "batch [60] loss: 29525499.87828682\n",
      "batch [70] loss: 29525499.87828682\n",
      "batch [80] loss: 29525499.87828682\n",
      "batch [90] loss: 29525499.87828682\n",
      "batch [100] loss: 29525499.87828682\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 29525499.87828682\n",
      "batch [10] loss: 29525499.87828682\n",
      "batch [20] loss: 29525499.87828682\n",
      "batch [30] loss: 29525499.87828682\n",
      "batch [40] loss: 29525499.87828682\n",
      "batch [50] loss: 29525499.87828682\n",
      "batch [60] loss: 29525499.87828682\n",
      "batch [70] loss: 29525499.87828682\n",
      "batch [80] loss: 29525499.87828682\n",
      "batch [90] loss: 29525499.87828682\n",
      "batch [100] loss: 29525499.87828682\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 29525499.87828682\n",
      "batch [10] loss: 29525499.87828682\n",
      "batch [20] loss: 29525499.87828682\n",
      "batch [30] loss: 29525499.87828682\n",
      "batch [40] loss: 29525499.87828682\n",
      "batch [50] loss: 29525499.87828682\n",
      "batch [60] loss: 29525499.87828682\n",
      "batch [70] loss: 29525499.87828682\n",
      "batch [80] loss: 29525499.87828682\n",
      "batch [90] loss: 29525499.87828682\n",
      "batch [100] loss: 29525499.87828682\n",
      "Using scale consts: [0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125]\n",
      "batch [0] loss: 29525499.87828682\n",
      "batch [10] loss: 29525499.87828682\n",
      "batch [20] loss: 29525499.87828682\n",
      "batch [30] loss: 29525499.87828682\n",
      "batch [40] loss: 29525499.87828682\n",
      "batch [50] loss: 29525499.87828682\n",
      "batch [60] loss: 29525499.87828682\n",
      "batch [70] loss: 29525499.87828682\n",
      "batch [80] loss: 29525499.87828682\n",
      "batch [90] loss: 29525499.87828682\n",
      "batch [100] loss: 29525499.87828682\n",
      "Using scale consts: [0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625, 0.00625]\n",
      "batch [0] loss: 29525499.87828682\n",
      "batch [10] loss: 29525499.87828682\n",
      "batch [20] loss: 29525499.87828682\n",
      "batch [30] loss: 29525499.87828682\n",
      "batch [40] loss: 29525499.87828682\n",
      "batch [50] loss: 29525499.87828682\n",
      "batch [60] loss: 29525499.87828682\n",
      "batch [70] loss: 29525499.87828682\n",
      "batch [80] loss: 29525499.87828682\n",
      "batch [90] loss: 29525499.87828682\n",
      "batch [100] loss: 29525499.87828682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 1.1262e-10,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8241e-13,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 6.6514e-10,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [1.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 1.8929e-14,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_attack = L2Adversary(\n",
    "    targeted=False, confidence=0.0, c_range=(1e-1, 1e10),\n",
    "    search_steps=5, max_steps=1000, abort_early=True,\n",
    "    box=(0., 1.), optimizer_lr=1e-2, init_rand=True\n",
    ")\n",
    "advxs = None\n",
    "all_inputs = []\n",
    "all_advx = []\n",
    "\n",
    "for data, target in test_loader:\n",
    "    if len(all_advx) * batch_size >= 500:\n",
    "        break\n",
    "    \n",
    "    print(\"OG Labels: \", target)\n",
    "    target = F.softmax(model(data.to(device)), dim=1)\n",
    "    print(\"Model pred: \", target)\n",
    "    target = torch.argmax(target, dim=1)\n",
    "    print(target)\n",
    "    data = data.view(batch_size, -1)\n",
    "    with open('input.npz', 'wb') as file:\n",
    "        all_inputs.append(sparse.csr_matrix(data))\n",
    "        sparse.save_npz(file, sparse.csr_matrix(data))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    advxs = cw_attack(model, data, target, to_numpy=False)\n",
    "    sparse_advxs = sparse.csr_matrix(torch.round(advxs), dtype='i1')\n",
    "    all_advx.append(sparse_advxs)\n",
    "    \n",
    "    with open('advxs.npz', 'wb') as file:\n",
    "        sparse.save_npz(file, sparse_advxs)\n",
    "        \n",
    "all_inputs = sparse.vstack(all_inputs)\n",
    "all_advx = sparse.vstack(all_advx)\n",
    "    \n",
    "advxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fixed-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_folder = os.path.join('data', 'out', 'all-apps', 'attack')\n",
    "os.makedirs(attack_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(attack_folder, 'inputs.npz'), 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(all_inputs))\n",
    "\n",
    "with open(os.path.join(attack_folder, 'advxs.npz'), 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(all_advx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('advxs.npz', 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(advxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-movie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facial-warner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(torch.abs(advxs - 0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comfortable-genetics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(F.softmax(model(torch.round(advxs.double().to(device))), dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "posted-letters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004719046355192229"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = sparse.csr_matrix(torch.round(advxs))\n",
    "density = mat.getnnz() / np.prod(mat.shape)\n",
    "density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "passive-better",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55096668"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.data.nbytes + mat.indptr.nbytes + mat.indices.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "junior-polls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.35703"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2535703 * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "floppy-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('advxs.npz', 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(advxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "framed-kuwait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [0., 1.])\n",
    "pd.Series(to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [0., 1.]).tolist(), \n",
    "          index=np.arange(-1,1,.001)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "appropriate-stuart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor(np.random.randint(0,2, (3,4)))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "concrete-great",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf, inf],\n",
       "        [inf, inf, inf, -inf],\n",
       "        [inf, inf, -inf, inf]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tanh_space(inputs, [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cathedral-sunrise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10., -10., -10.,  inf],\n",
       "        [ inf,  inf,  inf, -10.],\n",
       "        [ inf,  inf, -10.,  inf]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(to_tanh_space(inputs, [0., 1.]), min=-1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "close-turkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_tanh_space(torch.clamp(to_tanh_space(inputs, [0., 1.]), min=-1e4), [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "immune-gabriel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(inputs + from_tanh_space(pert_tanh*1000, [0., 1.]), max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "victorian-banking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9125,  1.7148, -0.6042,  0.3287],\n",
       "        [ 0.8110,  0.7492, -0.4228, -0.1139],\n",
       "        [-0.0307,  0.3254,  0.4284,  1.6197]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_tanh = torch.zeros((3,4))  # type: torch.FloatTensor\n",
    "nn.init.normal_(pert_tanh, mean=0, std=1)\n",
    "pert_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "studied-beijing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 1.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_tanh_space(pert_tanh*1e4, [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "checked-testimony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(inputs + from_tanh_space(pert_tanh*1e4, [0., 1.]), max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "continental-philippines",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(from_tanh_space(\n",
    "    (to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [-1., 1.])*1000), [-1., 1]).tolist(),\n",
    "    index=to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [-1., 1.]).tolist()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ruled-blues",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(from_tanh_space(torch.Tensor(np.arange(-1000, 1000, 1)), box=[0,1]).tolist(),\n",
    "    index=np.arange(-1000, 1000, 1)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "great-navigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9990], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_tanh_space(to_tanh_space(torch.Tensor([0,.999]).double(), [-1., 1.]), [-1., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # Training settings\n",
    "#     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "#     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "#                         help='input batch size for training (default: 64)')\n",
    "#     parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "#                         help='input batch size for testing (default: 1000)')\n",
    "#     parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "#                         help='number of epochs to train (default: 14)')\n",
    "#     parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "#                         help='learning rate (default: 1.0)')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "#                         help='Learning rate step gamma (default: 0.7)')\n",
    "#     parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                         help='disables CUDA training')\n",
    "#     parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "#                         help='quickly check a single pass')\n",
    "#     parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                         help='random seed (default: 1)')\n",
    "#     parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                         help='how many batches to wait before logging training status')\n",
    "#     parser.add_argument('--save-model', action='store_true', default=False,\n",
    "#                         help='For Saving the current Model')\n",
    "#     args = parser.parse_args()\n",
    "#     use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "#     torch.manual_seed(args.seed)\n",
    "\n",
    "#     device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#     train_kwargs = {'batch_size': args.batch_size}\n",
    "#     test_kwargs = {'batch_size': args.test_batch_size}\n",
    "#     if use_cuda:\n",
    "#         cuda_kwargs = {'num_workers': 1,\n",
    "#                        'pin_memory': True,\n",
    "#                        'shuffle': True}\n",
    "#         train_kwargs.update(cuda_kwargs)\n",
    "#         test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "#     # load data (will need to be adapted as well)\n",
    "#     # 1) load A_test\n",
    "#     # 2) load labels\n",
    "#     # 3) Perform train-test-split\n",
    "# #     transform=transforms.Compose([\n",
    "# #         transforms.ToTensor(),\n",
    "# #         transforms.Normalize((0.1307,), (0.3081,))\n",
    "# #         ])\n",
    "# #     dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "# #                        transform=transform)\n",
    "# #     dataset2 = datasets.MNIST('../data', train=False,\n",
    "# #                        transform=transform)\n",
    "#     dataset = HindroidDataset(\n",
    "#         'data/out/all-apps/hindroid-train-half/A_test.npz', \n",
    "#         'data/out/all-apps/hindroid-train-half/predictions.csv',\n",
    "#         'AAT'\n",
    "#     )\n",
    "#     train_loader = torch.utils.data.DataLoader(dataset,**train_kwargs)\n",
    "#     test_loader = torch.utils.data.DataLoader(dataset, **test_kwargs)\n",
    "\n",
    "#     model = HindroidSubstitute().to(device)\n",
    "#     optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "#     scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "#     for epoch in range(1, args.epochs + 1):\n",
    "#         train(args, model, device, train_loader, optimizer, epoch)\n",
    "#         test(model, device, test_loader)\n",
    "#         scheduler.step()\n",
    "\n",
    "#     if args.save_model:\n",
    "#         torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
